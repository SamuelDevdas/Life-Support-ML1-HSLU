---
title: "Life Support"
author: "Samuel Devdas, Pradip Ravichandran"
date: "2024-01-12"
output:
  prettydoc::html_pretty:
    theme: cayman
    css: styles.css
    highlight: github
    df_print: paged
    toc: true
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

In the bustling world of data science, Pradip Ravichandran and Samuel Devdas, two ambitious students, found themselves immersed in a challenging project. Their mission? To unravel the intricacies of a dataset from <https://archive.ics.uci.edu/dataset/880/support2> comprising 9105 critically ill patients across five United States medical centers, spanning the years 1989 to 1994. This treasure trove of information aimed to shed light on the 2- and 6-month survival rates of patients grappling with various critical illnesses.

# Import Library

The following libraries were used in this work:

```{r LS_2, echo=TRUE, ,include=TRUE, warning=FALSE, message=FALSE}
#library(Amelia)       # Missing data handling and imputation
#library(caret)
library(dplyr)         # To use pipes
#library(e1071)
#library(ggplot2)      # Visualization and graphics
#library(mgcv)         # Fitting GAM
#library(neuralnet)
#library(readr)        # Data file reading and parsing
library(reshape2)
#library(tidyverse)    # Data manipulation and visualization
```

# Load Data Set

According to the HBiostat Repository (<https://hbiostat.org/data/repo/supportdesc>, Professor Frank Harrell) the following default values have been found to be useful in imputing missing baseline physiologic data:

| Baseline Variable   | short | Default Value |
|---------------------|-------|---------------|
| Serum albumin       | alb   | 3.5           |
| PaO2/FiO2 ratio     | pafi  | 333.3         |
| Bilirubin           | bili  | 1.01          |
| Serum Creatinine    | crea  | 1.01          |
| Blood urea nitrogen | bun   | 6.51          |
| White blood count   | wblc  | 9             |
| Urine output        | urine | 2502          |

And following predictors should be removed, as they are findings from a previous model:

| Baseline Variable                             | short  |
|-----------------------------------------------|--------|
| APACHE III day 3 physiology score             | aps    |
| Patient had DNR order                         | dnr    |
| Day of DNR order (<0 if before study)         | dnrday |
| Physician’s 2-month survival estimate for pt. | prg2m  |
| Physician’s 6-month survival estimate for pt. | prg6m  |
| SUPPORT day 3 physiology score                | sps    |
| SUPPORT model 2-month survival estimate       | surv2m |
| SUPPORT model 6-month survival estimate       | surv6m |


```{r LS_3, echo=TRUE}
data <- read.csv("support2.csv", header = TRUE, sep = ",")
data_stage1 <- data

# Remove the mention columns
data[ , c("aps", "dnr", "dnrday", "prg2m", "prg6m", "sps", "surv2m", "surv6m")] <- list(NULL)

# Not relevant for our study:
# urine, sod and hrt are physiological measure, we decided to keep urine, because its easiest to interpret 
# !!!! also removed urine, maybe rethink !!!!
# dzclass, dzgroup and ca has similar information, we will keep dzgroup
# Redundant or less impactful variables: 'diabetes', 'd.time', 'death', 'race' 'resp', 'sex', 'wblc'
data[ , c("avtisst", "ca", "diabetes", "death", "d.time", "dzclass", "hrt", "race", "resp", "scoma", "sex", "sod", "urine", "wblc")] <- list(NULL)


# Create a named vector with default values for each variable
default_values <- c(alb = 3.5, pafi = 333.3, bili = 1.01, crea = 1.01, bun = 6.51)

# Loop through the vector to replace missing value
for (var in names(default_values)) {
    data[[var]][is.na(data[[var]])] <- default_values[var]
}

```

# Data Analyse

## Inspection

First in line was Samuel, armed with a passion for data analysis. He delved into the dataset, exploring each variable -- from demographic details like age, sex, and education level to disease categories and severity indicators.

```{r LS_4_1, echo=FALSE}
str(data)
source("DA.R")
```

After Inspecting closely, We see that some categorical variables datatype are not factors eg. `$ dzgroup : chr  "Lung Cancer" "Cirrhosis" "ARF/MOSF w/Sepsis" "Coma" ...`

```{r LS_4_1_1, echo=FALSE}
str(unique_values)
```

## Data Cleaning and Preprocessing

He meticulously examined missing values, ensuring a clean and reliable dataset for further analysis.

```{r LS_4_2, echo=FALSE}
str(data_stage2[cat_interest])
```

We have only `r sum(complete.cases(data_stage2))` complete cases *(rows with no missing values)* out of `r dim(data_stage2)[1]` rows. This is a very small number of complete cases. In order to make the dataset usable, we will have to find identify which variables have the most missing values and drop them from the dataset.\
Variables with the most missing values are:

`r data_stage2 %>% summarise_all(~ sum(is.na(.))) %>% stack() %>% subset(values > 0) %>% arrange(desc(values))`

Removing the variables with more than 65% missing values *(adlp, adls, glucose, totmcst)*, will help to get a complete dataset.
To determine further, which variables to drop, we will create a correlation matrix and identify the variables with high correlation.

`r head(top_correlations, 20)`

We find that the following variables are highly correlated with other variables:

1. totcst : charges (Cost to charge ratio : Total charges) - (Cor = 0.87)
3. totcst : slos (Cost to charge ratio : Days until discharge) - (Cor = 0.78)
4. charges : slos (Total charges : Days until discharge) - (Cor = 0.62)
5. charges : hday (Total charges : Day in hospital) - (Cor = 0.51)

*totcst* has the highest and second highest correlation, by removing it, it can help to avoid multicollinearity issue in regression model. Additionally *totcst* has 888 missing value.

After removing *(adlp, adls, glucose, totcst, totmcst)*, we still have only `r sum(complete.cases(data_stage3))` complete cases.\

### Imputing Missing Values

Impute other variables:

-   income *(replace missing value with the mode value)*
-   edu *(replace missing value with the mode value)*
-   ph *(replace missing value with the mean value)*

We will drop the 'rows' with missing values for the following variables:

1. charges
2. meanbp
3. temp
4. sfdm2

Samuel's keen eye uncovered patterns and relationships, providing a solid foundation for the subsequent stages of their project.
```{r LS_4_3, echo=FALSE}
sorted_sums <- colSums(is.na(data_stage3))
stack(sort(sorted_sums[sorted_sums != 0], decreasing = TRUE))
```

Finally we have `r sum(complete.cases(data))` complete cases *(rows with no missing values)* out of `r dim(data)[1]` rows and `r dim(data)[2]` columns.

To achieve our regulation and goal, we reduced `r dim(data_stage1)[2] - dim(data)[2]` variables and `r dim(data_stage1)[1] - dim(data)[1]` rows

# Linear Model

Emboldened by the insights from Samuel's analysis, the duo ventured into building a Linear Model. Samuel, with his flair for mathematical precision, took the lead. He meticulously crafted a model that encapsulated the essence of the dataset, laying the groundwork for predicting survival rates based on physiological and demographic features.

```{r LS_5, echo=FALSE}
#source("LM.R")
```

# Generalized Linear Models

But the journey didn't stop there. Samuel, driven by a thirst for deeper understanding, delved into the realm of Generalized Linear Models (GLMs). With the family set to Poisson, he explored the nuances of count data, bringing a new dimension to their predictive capabilities. The exploration continued with the family set to Binomial, unraveling the mysteries of binary outcomes.

```{r LS_6_1, echo=FALSE}
#source("GLM Poisson.R")
```

```{r LS_6_2, echo=FALSE}
#source("GLM Binomial.R")
```

# Generalized Additive Model

As the baton passed to Pradip, the project took a nonlinear turn. Pradip, with an affinity for complexity, introduced a Generalized Additive Model (GAM). This allowed them to capture non-linear relationships, providing a more nuanced understanding of the factors influencing patient survival.

```{r LS_7, echo=FALSE}
#source("GAM.R")
```

# Neural Networks

In the ever-evolving landscape of data science, Pradip took the reins for the next leg -- the realm of Neural Networks. With an aptitude for the intricacies of artificial intelligence, he crafted a network that could learn and adapt, pushing the boundaries of predictive accuracy.

```{r LS_8, echo=FALSE}
#source("NN.R")
```

# Support Vector Machine

The journey continued with Pradip steering towards a Support Vector Machine (SVM). As the data danced through hyperplanes, Pradip fine-tuned the model to discern the subtle boundaries between survival and adversity.

```{r LS_9, echo=FALSE}
#source("SM.R")
```

# Optimization problem

Yet, the dynamic duo was not content to rest on their laurels. Samuel and Pradip united their skills to tackle an optimization problem head-on. With a fusion of mathematical prowess and computational finesse, they sought to optimize a critical aspect of the project, bringing it to new heights.

# Conclusion

In the year 2023, Pradip Ravichandran and Samuel Devdas stood at the intersection of traditional statistical models and cutting-edge machine learning techniques. Their journey through linear, generalized, and additive models, neural networks, and support vector machines painted a vivid picture of the power that data science held in transforming raw information into actionable insights. As they solved optimization problems in unison, their story echoed the collaborative spirit essential in the world of data science, where the fusion of diverse skills creates a symphony of knowledge and innovation.
