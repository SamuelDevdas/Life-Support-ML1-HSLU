---
title: "Predictive Analysis of Critical Care Costs: Leveraging Machine Learning to Understand Healthcare Factors in Life Support Patients"
author:
  - Samuel Devdas
  - Pradip Ravichandran
date: "2024-01-12"
output:
  prettydoc::html_pretty:
    theme: cayman
    css: styles.css
    highlight: github
    df_print: paged
    toc: true
    number_sections: true
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, results = 'markup')
```

## Abstract

This report details a comprehensive study conducted by Pradip Ravichandran and Samuel Devdas, focusing on the application of machine learning in healthcare economics and patient care. The study utilizes a dataset comprising 9105 critically ill patients from five U.S. medical centers, recorded between 1989 and 1994. The dataset, accessible at [Support2 Dataset](https://archive.ics.uci.edu/dataset/880/support2), forms the foundation for developing machine learning models aimed at predicting medical charges, the probability of death, and the number of comorbidities for patients on life support.

### Model Development Contributions

- **Samuel Devdas** has led the development of models included in this report, namely:
  1. Linear Models : Multiple Linear Regression, Interaction Terms, and Non-Linearity
  2. GAM Models : Generalized Additive Models
  3. GLM Models : Logistic Regression and Poisson Regression

- **Pradip Ravichandran** has taken the lead in advancing the following sophisticated models:
  1. Artificial Neural Networks (ANN)
  2. Support Vector Machines (SVM)



---


## Introduction

### Background and Significance

The significance of this research lies in its broad application across various critical domains, including:

1. **Financial Management:** The study seeks to improve budgeting and resource allocation in healthcare by employing predictive analytics for more efficient financial planning.
2. **Insurance Processing:** These models are designed to forecast potential expenses, offering vital insights for patients in managing insurance and out-of-pocket costs.
3. **Cost-Effectiveness:** An analysis focusing on the sustainability and practicality of life support treatments to aid in informed healthcare decision-making.
4. **Policy and Research Implications:** The outcomes of this research are anticipated to inform healthcare policy and economic research, thereby influencing the standard and accessibility of medical care.
5. **Ethical and Patient-Centric Considerations:** A key goal is to balance fiscal concerns with ethical considerations and patient-focused care in life support scenarios.

---

## Data Description and Inspection

According to the HBiostat Repository (<https://hbiostat.org/data/repo/supportdesc>, Professor Frank Harrell) the following default values have been found to be useful in imputing missing baseline physiologic data:

| Baseline Variable   | short | Default Value |
|---------------------|-------|---------------|
| Serum albumin       | alb   | 3.5           |
| PaO2/FiO2 ratio     | pafi  | 333.3         |
| Bilirubin           | bili  | 1.01          |
| Serum Creatinine    | crea  | 1.01          |
| Blood urea nitrogen | bun   | 6.51          |


And following predictors should be removed, as they are findings from a previous model:

| Baseline Variable                             | short  |
|-----------------------------------------------|--------|
| APACHE III day 3 physiology score             | aps    |
| Patient had DNR order                         | dnr    |
| Day of DNR order (<0 if before study)         | dnrday |
| Physician’s 2-month survival estimate for pt. | prg2m  |
| Physician’s 6-month survival estimate for pt. | prg6m  |
| SUPPORT day 3 physiology score                | sps    |
| SUPPORT model 2-month survival estimate       | surv2m |
| SUPPORT model 6-month survival estimate       | surv6m |


---


## Dataset Preparation for Predictive Modeling

### Data Refinement Process

To ensure the efficacy of the predictive models, a rigorous dataset refinement process was undertaken. This process involved the following key steps:

1. **Removing Post-Admission Metrics**: 
   - **Rationale**: The study's focus is on initial patient assessments; therefore, variables measured post-admission were deemed irrelevant.
   - **Variables Excluded**: 'aps', 'dnr', 'dnrday', and 'surv2m'.

2. **Omitting Variables with Limited Predictive Value**: 
   - **Rationale**: Some variables were found to offer limited predictive value or were redundant.
   - **Variables Removed**: 'sod' (Sodium levels), 'ca' (Cancer), and 'urine'.

3. **Discarding Redundant or Low-Impact Variables**: 
   - **Rationale**: The goal was to streamline the dataset by eliminating variables that had minimal impact on the predictive models or were redundant.
   - **Variables Eliminated**: 'diabetes', 'dzclass', 'hrt', 'd.time', 'death', and 'resp'.

### Resulting Dataset

After this refinement process, the dataset was narrowed down to 20 key variables. This optimization aimed at enhancing the dataset's suitability for effective predictive modeling, thereby improving the accuracy and relevance of the study's findings.

<details>
<summary>Click for Code and Outputs </summary>
```{r}
# Load dataset
df_support <- read.csv("support2.csv", header = TRUE, sep = ",")

# Check the Variables' descriptions
df_description <- read.csv("support-variables-description.csv", header = TRUE, sep = ",")
df_description

# Inspect dataset
head(df_support, 3)

# Check structure of the dataset
str(df_support)
dim(df_support)

# Subset the data frame
selected_vars <- c("age", "hospdead", "slos", "dzgroup", "num.co", 
                   "edu", "income", "scoma", "charges", "hday", 
                   "dementia", "meanbp", "temp", "pafi", "alb", 
                   "bili", "crea", "ph", "bun", "adlsc")

# Create a new data frame with only the selected variables
df_support <- df_support[selected_vars]
```
<details>

---

## Data Cleaning and Preprocessing

### Overview

Data cleaning and preprocessing are critical steps in preparing a dataset for predictive modeling. In this phase, we focused on converting data into a format that can be effectively utilized in machine learning models, with particular attention to correctly formatting and processing categorical variables.

### Process Description

#### Categorical Variables Inspection

Our initial step involved a detailed inspection of the dataset, particularly focusing on categorical variables. We identified issues with data types of some variables. For instance, the `income` variable was incorrectly classified as a character (`chr`) type, which should have been a categorical (`factor`) type. This misclassification can impact the model's performance as it treats these variables inappropriately. 

- Example of Incorrect Data Type: `income: chr "$11-$25k" "$11-$25k" "under $11k"`

#### Code Implementation

1. **Identification of Categorical Variables:**
   We began by grouping the variables of interest (`hospdead`, `dzgroup`, `income`, `dementia`) which are categorical in nature.

2. **Verification of Unique Values:**
   Using the `sapply` function, we checked for unique values in these variables within the dataset to understand their structure and variability.

3. **Conversion to Factors:**
   To correct the data types, we converted these variables into factors. Notably, `income` was converted to an ordered factor to maintain the inherent order in income categories. This conversion is crucial for models that are sensitive to the ordinal nature of the data.

4. **Final Structure Validation:**
   After the conversion, we rechecked the structure of the dataset to ensure that the changes were correctly implemented.


<details>
<summary>Click for Code and Outputs </summary>
```{r}
# Identify and Group the categorical variables as categories of interest
cat_interest <- c("hospdead", "dzgroup", "income", "dementia")

# Apply unique function to validate that the cat_interest have unique categorical values and check their structure
unique_values <- sapply(df_support[cat_interest], unique)
str(unique_values)

# Convert categorical and ordinal variables to factors
df_support$hospdead <- factor(df_support$hospdead)
df_support$dzgroup <- factor(df_support$dzgroup)
df_support$income <- factor(df_support$income, ordered = TRUE, 
                            levels = c("under $11k", "$11-$25k", "$25-$50k", ">$50k"))
df_support$dementia <- factor(df_support$dementia)

# Check structure of dataset to confirm changes
str(df_support[cat_interest])

```
<details>

---

## Imputing Missing Values

### Introduction

Handling missing data is a crucial step in the machine learning pipeline. Inconsistent handling of missing values can introduce bias or distort the underlying patterns in the data, impacting model performance.

### Methodology

1. **Predefined Imputation**: For certain variables (`alb`, `pafi`, `bili`, `crea`, `bun`), we impute missing values with predefined constants. This approach is often used when domain knowledge suggests a standard value or when a typical value is known. 

2. **Mean Imputation for 'ph'**: We impute the variable `ph` with the mean of its non-missing values. Mean imputation is effective for normally distributed data or when the proportion of missing data is low.

3. **Mode Imputation for Categorical Variables**: For categorical variables (`income`, `edu`), we employ mode imputation. This method is suitable for categorical data, substituting missing values with the most frequently occurring category.

4. **Validation of Imputation**: Post-imputation, we verify the absence of missing values in our dataset. This step ensures data integrity and completeness.

5. **Data Cleaning**: Finally, we remove any remaining rows with missing values. This step is vital in scenarios where imputation is not feasible or when the remaining missing data is minimal and unlikely to impact the dataset's overall quality.


<details>
<summary>Click for Code and Outputs </summary>
```{r}
# Impute missing values with specified values
impute_values <- c(
    alb = 3.5, pafi = 333.3, bili = 1.01,
    crea = 1.01, bun = 6.51
)

# Loop through the vector to impute values
for (var in names(impute_values)) {
    df_support[[var]][is.na(df_support[[var]])] <- impute_values[var]
}

# Impute the variable "ph" with the mean value
mean_ph <- mean(df_support$ph, na.rm = TRUE)
df_support$ph[is.na(df_support$ph)] <- mean_ph

# Impute the variable "income" with the mode value
getMode <- function(v) {
    uniqv <- unique(na.omit(v))
    uniqv[which.max(tabulate(match(v, uniqv)))]
}
mode_value <- getMode(df_support$income)
df_support$income[is.na(df_support$income)] <- mode_value

# Impute the variable "edu" with the mode value
mode_value <- getMode(df_support$edu)
df_support$edu[is.na(df_support$edu)] <- mode_value

# Check for Missing Values in df_support and sort in descending order
na_counts_col <- colSums(is.na(df_support))
na_counts_col <- sort(na_counts_col, decreasing = TRUE)
na_counts_col[1:15]

# Drop all the remaining rows with missing values.
final_df <- df_support[complete.cases(df_support), ]
dim(final_df)
sort(colnames(final_df))
```
<details>

---

## Exploratory Data Analysis

### Distribution Analysis of 'Charges'

#### Initial Observation
- The initial analysis of the `charges` variable reveals a highly right-skewed distribution. This skewness indicates that the majority of charges are less than 100,000, suggesting a concentration of lower values in our dataset.

#### Transformation Strategy
- To address the skewness and linearize the relationships, we apply a logarithmic transformation to the `charges` variable. This approach is standard in data analysis to handle skewed data distributions.

#### Implementation Details
1. **Logarithmic Transformation**:
   - The logarithm of zero or negative numbers is undefined. To circumvent this issue, we add a small constant (1 in this case) to the `charges` before applying the logarithmic transformation. This step ensures that all values are positive and suitable for the transformation.
   - `final_df$log_charges <- log(final_df$charges + 1)`

2. **Re-evaluation of Distribution**:
   - Post-transformation, the distribution of `log_charges` is examined. The histogram of log-transformed charges indicates a shift towards a more normal distribution. This transformation makes the data more suitable for various statistical and machine learning models that assume normality of features.


<details>
<summary>Click for Code and Outputs </summary>
```{r}
hist(final_df$charges, breaks = 50, main = "Histogram of Charges", xlab = "Charges", col = "blue", border = "black")

# The "charges" variable is highly right skewed with majority <100000

# To linearize the relationships, we take the logarithm of the 'charges' variable. However, since the log of zero or negative numbers is undefined, we add a small constant before taking the log to avoid errors. We then check the distribution of the resulting log-transformed charges.

# Taking the logarithm of 'charges' to linearize relationships
final_df$log_charges <- log(final_df$charges + 1)

# Plotting the histogram to check the distribution of log-transformed 'charges'
hist(final_df$log_charges, breaks = 50, 
     main = "Histogram of log_Charges", 
     xlab = "Charges", col = "blue", border = "black")
```


### Data Visualization: Exploring Relationships in Healthcare Data

#### Objective

The primary aim here is to visually explore the relationships between various numeric variables and `log_charges`, a logarithmic transformation of medical charges, in a healthcare dataset. This examination is pivotal in identifying potential predictors for healthcare costs.

#### Preparing Data for Plotting

- **Selection of Numeric Variables**: We began by isolating numeric variables from the dataset. These variables represent different quantitative aspects of healthcare data.

- **Excluding Target Variables**: From this numeric subset, we intentionally excluded  'charges'. 'log_charges' is our variable of interest, against which we plot other variables, and 'charges' is its non-logarithmic counterpart.

#### Key Observations

- **Variables such as 'slos', 'edu', 'scoma', 'hday', 'hrt', 'temp', 'bili', 'crea', and 'bun' showed a positive relationship with 'log_charges'**. This indicates that as these variables increase, there's a tendency for the logarithm of medical charges to increase as well.

- **Variables like 'age', 'num.co', and 'pafi' exhibited a negative relationship with 'log_charges'**. This suggests that higher values of these variables are associated with lower logarithmic medical charges.


<details>
<summary>Click for Code and Outputs </summary>
```{r}
# Make data ready for plotting
numeric_vars <- names(final_df)[sapply(final_df, is.numeric)]
numeric_vars

# Exclude 'log_charges' from Numeric Columns
numeric_vars <- numeric_vars[!(numeric_vars %in% c("log_charges", "charges"))]
numeric_vars

# Loop through each numeric variable and create a scatterplot against 'log_charges'
library(ggplot2)
for (var in numeric_vars) {
  print(
    ggplot(final_df, aes_string(x = var, y = "log_charges")) +
      geom_point(shape = 19, size = 1, alpha = 0.5, color = "blue") +
      geom_smooth(method = "lm", se = FALSE, color = "red") +  # Add a linear regression line
      labs(title = paste("Scatterplot of log_charges vs", var), x = var, y = "log_charges")
  )
}
```
<details>

---

## Multiple Linear Regression: Predicting Charges for Life Support Patients

### Identifying Key Variables Influencing Life Support Patient Charges

#### Overview

In this section, we discuss the development of multiple linear models to identify the most significant variables impacting the 'log_charges' for life support patients. This is a crucial aspect of healthcare management, as understanding these variables can aid in cost prediction and management.

#### Model Development Process

1. **Initial Model Creation**: We start by fitting a linear model (`lm.charges1`) using all available variables except for 'charges'. This model serves as a baseline to identify potential predictors of 'log_charges'.

2. **Iterative Model Refinement**: 
   - We employ a stepwise approach, systematically removing variables that show the least significance based on their p-values and F-test results. This process involves updating the model multiple times (from `lm.charges1` to `lm.charges8`) to refine its predictive accuracy.
   - In each step, variables like 'crea', 'adlsc', 'alb', 'pafi', and 'scoma', among others, are excluded based on statistical tests (`drop1` analysis). 
   - The aim is to simplify the model by keeping only those variables that significantly contribute to the prediction of 'log_charges'.

3. **Assessing Model Fit**:
   - Throughout this process, we continuously assess the model fit using summary statistics and Akaike Information Criterion (AIC). This helps in determining the balance between model complexity and explanatory power.
   - By the final model (`lm.charges8`), we reach a point where further simplification would lead to a loss of predictive power, indicating an optimal set of variables.

#### Insights and Decision-Making

- **Variable Importance**: This process reveals the most critical variables impacting 'log_charges'. By removing variables one by one and observing the changes in the model's performance, we identify those factors that have the most substantial influence.

- **Model Simplification**: The stepwise refinement underscores the importance of simplicity in model construction. By removing less significant variables, we enhance the model's interpretability and efficiency without compromising its predictive ability.

- **Methodological Considerations**: This iterative approach demonstrates a careful balance between statistical rigor and practical relevance. The decisions made at each step are grounded in statistical tests and the overarching goal of model optimization.

<details>
<summary>Click for Code and Outputs </summary>
```{r}
# Fitting the starting model using all the variables except 'charges'
# Select only numeric columns from the data frame
numeric_columns <- sapply(final_df, is.numeric)

# Create a new data frame without the 'charges' column
full_df <- subset(final_df, select = -charges) 

# Fit a linear model to predict log-transformed charges using all other variables
lm.charges1 <- lm(log_charges~ ., data = full_df)

# Display the summary of the model fit
summary(lm.charges1)

# Dropping the least significant variable from the model and testing the difference
drop1(lm.charges1, test = "F")

# Updating the model by excluding identified variables
lm.charges2 <- update(lm.charges1, . ~ . - crea)
summary(lm.charges2)

# Dropping variables with higher p-values and inspecting the AIC impact
drop1(lm.charges2, test = "F")

# Simplifying the model based on the drop1 analysis and keeping variables of interest
# Removing the variables adlsc, alb, pafi, and scoma from the model
lm.charges3 <- update(lm.charges2, . ~ . -adlsc -alb - pafi -scoma )
summary(lm.charges3)

# Dropping variables one by one from the model lm.charges3 and testing with F-test
drop1(lm.charges3, test = "F")

# Updating the model lm.charges3 by removing the variables 'ph', 'dementia', 'num.co', 'meanbp'
# and then summarizing the new model lm.charges4
lm.charges4 <- update(lm.charges3, . ~ . - ph - dementia - num.co - meanbp )
summary(lm.charges4)

# Testing the model by dropping one variable at a time
drop1(lm.charges4, test = "F")

# Updating the model by removing the 'slos' variable
lm.charges5 <- update(lm.charges4, . ~ . - slos )
summary(lm.charges5)

# Performing a drop1 test on the model lm.charges5 to check which variable can be dropped
drop1(lm.charges5, test = "F")

# Updating the model by dropping the 'income' variable due to its high p-value
lm.charges6 <- update(lm.charges5, . ~ . - income)
summary(lm.charges6)

# Conducting drop1 analysis to test individual model terms
drop1(lm.charges6, test = "F")

# Updating the model by dropping variables with higher p-values, in this case 'bili'
lm.charges7 <- update(lm.charges6, . ~ . - bili )
summary(lm.charges7)

# Conducting drop1 analysis to see the effect of dropping each variable on model fit
drop1(lm.charges7, test = "F")

# Updating the model by dropping the variable with the highest AIC impact
lm.charges8 <- update(lm.charges7, . ~ . - hday)
summary(lm.charges8)

# Conducting final drop1 analysis to check for any remaining insignificant variables.
drop1(lm.charges8, test = "F")

# Finally, the model cannot be further simplified without loss of predictive power.

```
<details>

### Linear Models: Cross-Validation and Best Model Selection

#### Overview

In this phase, we focus on the critical step of cross-validating various linear models to determine the most effective one. Cross-validation is a robust method to evaluate the performance of models on unseen data, thereby reducing the risk of overfitting and ensuring that the selected model generalizes well to new data.

#### Methodology

- **Cross-Validation Technique**: We employ a 10-fold cross-validation technique using the `caret` package in R. This method splits the dataset into ten parts, trains the model on nine parts, and validates on the remaining part. This process is repeated ten times, each time with a different part used for validation.

- **Models Evaluated**: Multiple linear regression models (`lm.charges1` to `lm.charges8`) are evaluated. Each model differs in terms of the features included or the way these features are processed.

#### Results and Interpretation

- **Performance Metrics**: We assess the models based on Root Mean Square Error (RMSE), R-squared, and Mean Absolute Error (MAE). Lower RMSE and MAE values indicate better model accuracy, while a higher R-squared value suggests better model fit.

- **Top Performing Models**: 
  - `lm.charges1` and `lm.charges2` showed the best accuracy with RMSE approximately 0.726 and R-squared around 0.676. 
  - `lm.charges3` exhibited slightly lower performance.
  - `lm.charges4` presented a decent balance between accuracy and model complexity (RMSE: 0.731, R-squared: 0.671).
  - `lm.charges5` indicated lower accuracy (RMSE: 0.904, R-squared: 0.497), hinting at potential overfitting issues.

#### Conclusion

Based on the cross-validation results, we select `lm.charges4` for further development. This model is chosen for its balance between simplicity and effectiveness, indicating a good trade-off between accuracy and model complexity. This decision aligns with the principle of Occam's Razor in model selection, favoring models that achieve similar performance with fewer parameters or simpler structures.


<details>
<summary>Click for Code and Outputs </summary>
```{r}
# Load the required package
library(caret)

# Setting a seed for reproducibility
set.seed(123)

# Create a function for cross-validation
cross_validate_model <- function(model, full_df, number_folds = 10) {
  # Extract the formula from the model
  model_formula <- formula(model)
  
  # Set up cross-validation control
  train_control <- trainControl(method = "cv", number = number_folds)
  
  # Train the model using cross-validation
  cv_model <- train(model_formula, data = full_df, method = "lm", trControl = train_control)
  
  # Return the results
  return(cv_model$results)
}

# Perform cross-validation for each of the remaining models
results1 <- cross_validate_model(lm.charges1, full_df)
results2 <- cross_validate_model(lm.charges2, full_df)
results3 <- cross_validate_model(lm.charges3, full_df)
results4 <- cross_validate_model(lm.charges4, full_df)
results5 <- cross_validate_model(lm.charges5, full_df)
results6 <- cross_validate_model(lm.charges6, full_df)
results7 <- cross_validate_model(lm.charges7, full_df)
results8 <- cross_validate_model(lm.charges8, full_df)

# Load libraries for printing tables
library(knitr)
library(DT)

# Rounding the values in the data frame
results_df <- data.frame(
  Model = c("lm.charges1", "lm.charges2", "lm.charges3", "lm.charges4",
            "lm.charges5", "lm.charges6", "lm.charges7", "lm.charges8"),
  RMSE = round(c(results1$RMSE, results2$RMSE, results3$RMSE, 
                 results4$RMSE, results5$RMSE, results6$RMSE, results7$RMSE, results8$RMSE), 3),
  Rsquared = round(c(results1$Rsquared, results2$Rsquared, 
                     results3$Rsquared, results4$Rsquared, results5$Rsquared, results6$Rsquared, results7$Rsquared, results8$Rsquared), 3),
  MAE = round(c(results1$MAE, results2$MAE, results3$MAE, 
                results4$MAE, results5$MAE, results6$MAE, results7$MAE, results8$MAE), 3)
)

# Display the results as a table
datatable(results_df, caption = 'Linear Model Development Cross-validation Results', 
          options = list(pageLength = 10 ))
```
<details>


### Analysis of Interaction Terms on Model 'lm.charges4'

#### Interaction Effects of Age and Disease Group on Healthcare Charges

1. **Impact of Age Across Different Disease Groups**:
   - We investigated how the impact of age on healthcare charges varies across different disease groups in the dataset. This analysis is particularly relevant to understand if aging influences the cost of healthcare differently for various medical conditions.
   - **Findings**:
     - For certain disease groups like Chronic Heart Failure (CHF), Coma, and Multiple Organ System Failure (MOSF) with Malignancy, there is a significant decrease in charges with age. This suggests that for these conditions, older patients might incur lower healthcare costs.
     - However, for other conditions such as Cirrhosis, Colon Cancer, Chronic Obstructive Pulmonary Disease (COPD), and Lung Cancer, no significant interaction effect was observed. This indicates that the impact of age on healthcare charges is consistent across patients of different ages for these diseases.

#### Interaction Effects of Hospital Death and Disease Group on Healthcare Charges

2. **Effect of Hospital Death on Charges Modified by Disease Type**:
   - The study also explored whether the effect of hospital death on healthcare charges is modified by the type of disease a patient has. This analysis aims to uncover any differential financial implications of hospital death across various disease conditions.
   - **Findings**:
     - The analysis revealed significant interactions between hospital death and specific disease groups. This implies that the financial impact of hospital deaths varies considerably depending on the patient's disease condition.
     - For instance, certain diseases may exhibit higher healthcare charges in cases of hospital deaths, possibly indicating more resource-intensive care or complications leading to such outcomes.
     - Conversely, other diseases might show less pronounced financial impacts in the event of hospital deaths, suggesting different care patterns or resource utilization.



<details>
<summary>Click for Code and Outputs </summary>
```{r}
# Adding an interaction term between 'age' and 'dzgroup' to the model
lm_age_dzgroup <- lm(log_charges ~ age * dzgroup 
                     + hospdead + edu + income
                     + hday + temp + bili + bun, 
                     data = full_df)

# Checking the summary of the model to see the interaction effect
summary_lm_age_dzgroup <- summary(lm_age_dzgroup)
print(summary_lm_age_dzgroup)

#Summary of Interaction Effects between Age and Disease Groups
#- CHF: Significant decrease in charges with age.
#- Coma: Significant decrease in charges with age.
#- MOSF w/Malignancy: Significant decrease in charges with age.
#- Cirrhosis, Colon Cancer, COPD, Lung Cancer: No significant interaction.

#2. Is the effect of hospital death on healthcare charges modified
#by the type of disease a patient has?
#Adding an interaction term between hospital stay length (hospdead) and disease group

# Fit a linear model with interaction between hospdead and dzgroup, controlling for other variables
lm_hospdead_dzgroup <- lm(log_charges ~ hospdead * dzgroup 
                                      + age + edu + income + hday 
                                      + temp + bili + bun, 
                                      data = full_df)

# Check the summary of the model to see the interaction effect
print(summary(lm_hospdead_dzgroup))

```
<details>


### Cross-Validation of Linear Models with Interaction Terms

In this section, we focus on evaluating the impact of interaction terms on the linear model 'lm.charges4'. Specifically, we perform cross-validation on two linear models incorporating interaction terms: Age-Disease Group and Hospdead-Disease Group. The goal is to compare these models with 'lm.charges4' to understand how interaction terms influence model performance.

#### Methodology

1. **Cross-Validation Procedure**: We conduct cross-validation for both interaction models. This approach helps us assess the model's performance across different subsets of the data, ensuring a robust evaluation.

2. **Models Evaluated**:
   - Age-Disease Group Interaction Model (`lm_age_dzgroup`).
   - Hospdead-Disease Group Interaction Model (`lm_hospdead_dzgroup`).

#### Results and Analysis

- **Model Performance Comparison**:
  - 'lm.charges4': RMSE = 0.732, Rsquared = 0.673, MAE = 0.571.
  - 'lm_age_dzgroup': RMSE = 0.901, Rsquared = 0.499, MAE = 0.718.
  - 'lm_hospdead_dzgroup': RMSE = 0.894, Rsquared = 0.507, MAE = 0.71.

- **Interpretation**:
  - The original model 'lm.charges4' outperforms the interaction models in all three metrics, indicating better predictive accuracy and fit, but addition of interaction terms help to understand the dynamics of specific age and disease groups.
 

<details>
<summary>Click for Code and Outputs </summary>
```{r}
# Set a seed for reproducibility
set.seed(123)

# Perform cross-validation for the Age-Disease Group Interaction Model
results_age_dzgroup <- cross_validate_model(lm_age_dzgroup, full_df)

# Perform cross-validation for the Hospdead-Disease Group Interaction Model
results_hospdead_dzgroup <- cross_validate_model(lm_hospdead_dzgroup, full_df)

# Rounding and creating a data frame for the results
results_df2 <- data.frame(
  Model = c("lm.charges4", "lm_age_dzgroup", "lm_hospdead_dzgroup"),
  RMSE = round(c(results4$RMSE, results_age_dzgroup$RMSE, results_hospdead_dzgroup$RMSE), 3),
  Rsquared = round(c(results4$Rsquared, results_age_dzgroup$Rsquared, results_hospdead_dzgroup$Rsquared), 3),
  MAE = round(c(results4$MAE, results_age_dzgroup$MAE, results_hospdead_dzgroup$MAE), 3)
)

# Displaying the results in a datatable
datatable(results_df2, caption = 'Selected Interaction Cross-validation Results', options = list(pageLength = 5))
```
<details>


### Non-Linear effect modelling using Linear Models

#### Visual Exploration of Non-Linear Trends

In this phase of our analysis, we focus on identifying non-linear relationships within our dataset. Non-linear models are often more complex and can capture patterns that linear models might miss. Our approach is to visually explore these potential non-linear trends among the numeric variables.

##### Methodology

- **Visual Analysis**: We use scatterplots with loess smoothers to visualize each numeric variable's relationship with `log_charges`. This method provides an intuitive way to detect non-linear trends.

- **Tools Used**: For the visual exploration, we employ `ggplot2`, a powerful visualization library in R. 

- **Process**: For each numeric variable, a scatterplot is generated. We overlay a loess smoother—a type of non-parametric regression method—on each plot. The smoother (illustrated as a red line with an orange confidence area) highlights the potential non-linear relationship between the variable and `log_charges`.

##### EDA Summary: Non-Linear Effects

- **Findings**: The visual exploration reveals several non-linear effects in the dataset:

  - **Probable Quadratic Effects**: Variables such as `meanbp`, `bun`, `ph`, and `slos` demonstrate quadratic-like effects.

  - **Polynomial Effects**: Other variables, including `temp`, `edu`, `hrt`, `hday`, `bili`, `num.co`, and `alb`, show patterns that suggest polynomial relationships.

  - **Linear Trends**: Interestingly, the variable `age` does not exhibit significant non-linear trends, indicating a possibly linear relationship with `log_charges`.

<details>
<summary>Click for Code and Outputs </summary>
```{r}
# Load the ggplot2 library for plotting
library(ggplot2)

# Loop through each numeric variable to create scatterplots with a loess smoother
for (var in numeric_vars) {
  # Creating a scatterplot with a smoother to detect non-linear trends
  print(
    ggplot(final_df, aes_string(x = var, y = "log_charges")) +
      geom_point(shape = 19, size = 1, alpha = 0.5, color = "blue") +
      geom_smooth(method = "loess", se = TRUE, color = "red", fill = "orange") +  # Loess smoother for non-linear trends
      labs(title = paste("Exploring Non-Linear Relationship of", var, "with log_charges"),
           x = var, y = "log_charges")
  )
}
```
<details>
  

### Iterating on the lm.charges4 Model: Incorporating Non-Linear Effects

In this stage of our linear model development, we focus on enhancing the lm.charges4 model by incorporating observed non-linear effects. This iterative process involves evaluating and adjusting the model to better capture the complexities in the data.

#### Building a Model with Non-Linear Effects

- **Model Enhancement**: We augment the existing linear model (lm.charges4) by introducing non-linear terms. This is based on the observation that certain variables exhibit non-linear relationships with the target variable.

- **Evaluation of Non-Linear Terms**:
    - `num.co`: Higher order (polynomial) terms were found to be insignificant, leading to a decision to revert this variable to a linear term.
    - `bili`: A cubic polynomial (`poly3`) was initially tested but proved insignificant, prompting a reduction to a quadratic term (`poly2`).
    - `edu` and `meanbp`: Similar to `num.co`, higher order terms did not contribute significantly, thus reverting to linear terms.

#### Simplifying the Model

- **Removing Insignificant Polynomial Terms**: Based on the analysis, we further simplify the model by removing polynomial terms for `num.co`, `bili`, and `meanbp`. Additionally, terms for `alb` and `ph` are also removed to reduce model complexity.

- **Rationale**: This simplification step aims to streamline the model, enhancing its interpretability and efficiency without compromising on predictive accuracy.

<details>
<summary>Click for Code and Outputs </summary>
```{r}
# Update the lm.charges4 model with non-linear terms
nonlinear.lm.charges4 <- update(lm.charges4, . ~ . -slos - meanbp - bun - ph - alb 
                                - temp - edu - hday - bili - num.co +
                                  poly(slos, 2) +
                                  poly(meanbp, 2) + 
                                  poly(bun, 2) +
                                  poly(ph, 2) +
                                  poly(alb, 2) +
                                  poly(temp, 3) + 
                                  poly(edu, 3) +
                                  poly(hday, 3) +
                                  poly(bili, 3) +
                                  poly(num.co, 3))

# Summary of the updated model
summary(nonlinear.lm.charges4)

#### Simplify model, remove poly terms for num.co, bili, meanbp


lm.nonlinear.trim <- update(nonlinear.lm.charges4, . ~ .  
                              - poly(num.co, 3) - poly(bili,3) - poly(meanbp, 2)
                              - poly(alb, 2) - poly(ph, 2)
                              )
summary(lm.nonlinear.trim)
```
<details>

### Cross-Validation and Comparison of Models with Non-Linearity

In this segment, we focus on the cross-validation and comparative analysis of non-linear models derived from the original linear model `lm.charges4`. Our goal is to understand how non-linear adaptations improve model performance and whether a more refined model (trimmed model) can maintain predictive accuracy with reduced complexity.

#### Non-Linearity Model Adaptations

1. **Nonlinear lm.charges4**: This model represents the first step in moving away from linear assumptions. The cross-validation results indicate a significant improvement over the linear version:
   - **RMSE (Root Mean Square Error)**: Reduced to 0.649 from 0.731 (linear model), indicating better accuracy.
   - **R-squared**: Increased to 0.741 from 0.671 (linear model), showing enhanced predictive power.

2. **Trimmed Nonlinear Model**: This version further simplifies the model by reducing the number of variables.
   - **Performance Metrics**: It achieves an RMSE of 0.65 and an R-squared of 0.742, almost mirroring the performance of the full non-linear model.

#### Comparative Insights

- **Enhanced Accuracy with Non-Linearity**: The shift to a non-linear approach markedly improves the model's accuracy, as evidenced by lower RMSE and higher R-squared values compared to the linear counterpart.

- **Balancing Complexity and Performance**: The Trimmed Nonlinear Model maintains similar levels of predictive accuracy as the full non-linear model, despite the reduction in variables. This suggests an effective balance between model simplicity and predictive capability.


<details>
<summary>Click for Code and Outputs </summary>
```{r}
# Perform cross-validation for nonlinear.lm.charges4
results_nonlinear_charges4 <- cross_validate_model(nonlinear.lm.charges4, full_df)

# Perform cross-validation for the final trimmed model
results_trimmed <- cross_validate_model(lm.nonlinear.trim, full_df)

# Combine the selected results into one data frame
selected_results_df <- data.frame(
  Model = c(
    "lm.charges4", "Nonlinear lm.charges4", "Trimmed Nonlinear Model"
  ),
  RMSE = round(c(
    results4$RMSE, results_nonlinear_charges4$RMSE, results_trimmed$RMSE
  ), 3),
  Rsquared = round(c(
    results4$Rsquared, results_nonlinear_charges4$Rsquared, results_trimmed$Rsquared
  ), 3),
  MAE = round(c(
    results4$MAE, results_nonlinear_charges4$MAE, results_trimmed$MAE
  ), 3)
)

# Display the selected results table
datatable(selected_results_df, caption = 'Selected Non-Linear Cross-validation Results Comparison', options = list(pageLength = 10))
```
<details>


---


## Generalized Additive Model (GAM) Overview

In this section, we examine the application of a Generalized Additive Model (GAM) to capture non-linear dynamics in our dataset. Our aim is to build upon the insights gained from our previously most performant model, "lm.nonlinear.trim," and explore how a GAM can provide a more nuanced understanding of the relationships between variables.

### Model Building with GAM

- **Model Foundation**: The GAM is constructed using variables from the "lm.nonlinear.trim" model. This approach ensures that we are building on a foundation of variables already proven to be significant in capturing non-linear patterns.

- **Non-Linear Relationships**: GAM allows us to model non-linear relationships between the predictors and the response variable. By employing smooth functions , we can flexibly fit these non-linear relationships.

#### Interpretation of GAM Summary

- **Parametric Coefficients**: The significant intercept value suggests a strong baseline level in the log-transformed response variable, indicating the importance of the constant component in the model.

- **Smooth Terms Significance**: Each smooth term (representing each variable) shows a degree of freedom (edf) significantly greater than 1, demonstrating that the relationships are indeed non-linear. The p-values of these terms are highly significant, reinforcing the importance of these non-linear dynamics in the model.

- **Model Performance Metrics**:
    - **Adjusted R-Squared (0.741)**: This high value indicates that a substantial proportion of the variance in the data is explained by the model.
   

<details>
<summary>Click for Code and Outputs </summary>
```{r, echo=FALSE}
# Load GAM library
library(mgcv)

# Using the numeric variables from lm.nonlinear.trim to build a base GAM
gam.model1 <- gam(log_charges ~ s(age) +
                                s(slos) + 
                                s(bun) +
                                s(alb) +
                                s(temp) + 
                                s(hday) +
                                s(bili) +
                                s(edu) ,
                              data = full_df)

# Summary of the GAM model
summary(gam.model1)
```
<details>

### Cross-Validation and Comparison of Non-Linear Models

#### Cross-Validation of the GAM Model

In this section, we assess the performance of the Generalized Additive Model (GAM) alongside other non-linear models using cross-validation techniques. Our goal is to identify the model that offers the best balance between accuracy and interpretability.

#### Cross-Validation Results Summary

1. **Linear Model (lm.charges4)**:
   - RMSE: 0.731, R-squared: 0.671, MAE: 0.57
   - This model provides a basic level of predictive accuracy and serves as a baseline for comparison.

2. **Nonlinear Linear Model (Nonlinear lm.charges4)**:
   - RMSE: 0.649, R-squared: 0.741, MAE: 0.502
   - Demonstrates improved accuracy over the basic linear model, highlighting the benefits of incorporating nonlinearity.

3. **Trimmed Nonlinear Model**:
   - RMSE: 0.647, R-squared: 0.742, MAE: 0.503
   - Offers a marginal improvement in accuracy over the full nonlinear model. Its simpler and more interpretable structure makes it an attractive option.

4. **Generalized Additive Model (GAM)**:
   - RMSE: 0.659, R-squared: 0.732
   - Shows competitive performance, slightly lagging behind the trimmed nonlinear model. Its flexibility in capturing complex patterns is noteworthy, though it may compromise interpretability.

#### Conclusion

- The **Trimmed Nonlinear Model** stands out as the optimal choice, striking a balance between high accuracy and model simplicity.
- The **GAM Model**, while demonstrating robust performance, falls short in terms of interpretability when compared to simpler models. Its complexity suggests potential overfitting, which is a critical aspect to consider in model selection.
- This analysis underscores the importance of evaluating models not just on accuracy metrics, but also on their complexity and interpretability, ensuring that the chosen model aligns with the overarching goals of the analysis.


<details>
<summary>Click for Code and Outputs </summary>
```{r}
# Load the 'mgcv' library
library(mgcv)

# Prepare Data for K-Fold Cross-validation
set.seed(123)  # for reproducibility
k <- 10  # number of folds
n <- nrow(full_df)
fold_ids <- sample(rep(1:k, length.out = n))

# Define a function to perform k-fold cross-validation
perform_cv_for_gam <- function(model, data, k) {
  # Initialize vectors to store performance metrics
  rmse_values <- numeric(k)
  r_squared_values <- numeric(k)

  # Generate fold IDs for cross-validation
  set.seed(123)  # For reproducibility
  fold_ids <- sample(rep(1:k, length.out = nrow(data)))

  # Perform k-fold cross-validation
  for (i in 1:k) {
    train_data <- data[fold_ids != i, ]
    test_data <- data[fold_ids == i, ]

    # Make predictions and calculate RMSE and R-squared
    predictions <- predict(model, test_data, type = "response")
    rmse_values[i] <- sqrt(mean((predictions - test_data$log_charges)^2))
    r_squared_values[i] <- cor(predictions, test_data$log_charges)^2
  }

  # Calculate and return the average RMSE and R-squared
  list(mean_rmse = mean(rmse_values), mean_r_squared = mean(r_squared_values))
}

# Results of 10-fold cross-validation
cv_results <- perform_cv_for_gam(gam.model1, full_df, 10)
gam_mean_rmse <- cv_results$mean_rmse
gam_mean_r_squared <- cv_results$mean_r_squared

# Combine the Selected Results into One Data Frame
selected_results_df <- data.frame(
  Model = c(
    "lm.charges4", "Nonlinear lm.charges4", "Trimmed Nonlinear Model", "GAM Model"
  ),
  RMSE = round(c(
    results4$RMSE, results_nonlinear_charges4$RMSE, results_trimmed$RMSE, gam_mean_rmse
  ), 3),
  Rsquared = round(c(
    results4$Rsquared, results_nonlinear_charges4$Rsquared, results_trimmed$Rsquared, gam_mean_r_squared
  ), 3),
  MAE = c(
    round(results4$MAE, 3), round(results_nonlinear_charges4$MAE, 3), round(results_trimmed$MAE, 3), NA
  )
)

# Display the selected results table
datatable(selected_results_df, caption = 'Selected GAM model Cross-validation Results Comparison', options = list(pageLength = 10))
```
<details>

------------------------------------------------------------------------

## Generalized Linear Models (GLM: Logistic Regression)

In this section, we explore the use of Generalized Linear Models (GLM) to identify key predictors influencing the likelihood of a patient's death in the hospital. The focus is on understanding which variables significantly contribute to this outcome.

### Methodology

1. **Data Visualization**: We employed scatterplots to visually inspect the relationship between each numeric variable and the 'hospdead' status (a binary indicator of patient death in the hospital). 

2. **Variable Selection**: Our analysis highlighted several variables, including `log_charges`, `num.co`, `age`, `adlsc`, and `alb`, that exhibit distinct separation patterns when plotted against the `hospdead` status. These variables are considered potential key predictors due to their noticeable patterns in the scatterplots.

### Observations

- **Visual Insights**: The scatterplots provided preliminary insights into the relationship between various numeric variables and patient mortality. Variables showing clear separation patterns between the different `hospdead` statuses suggest a stronger association with the outcome.

- **Potential Predictors**: Among the variables examined, `log_charges`, `num.co`, `age`, `adlsc`, and `alb` were identified as potentially significant predictors. These variables exhibited noticeable differences in distribution between patients who died and those who survived, as observed in the scatterplots.


<details>
<summary>Click for Code and Outputs </summary>
```{r, echo=FALSE}
# Make data ready for plotting
numeric_vars <- names(final_df)[sapply(final_df, is.numeric)]
numeric_vars

# Exclude 'log_charges' from Numeric Columns
numeric_vars <- numeric_vars[!(numeric_vars %in% c("charges"))]
numeric_vars

# Load the ggplot2 library
library(ggplot2)

# Set the sample size for better visibility of the scatterplots
sample_size <- 500  # Adjust this number based on your data size and needs

for (var in numeric_vars) {
  # Take a random sample of the data
  sampled_data <- final_df[sample(nrow(final_df), sample_size), ]
  
  # Create the scatterplot with the sampled data
  print(ggplot(sampled_data, aes_string(x = var, y = 'hospdead')) +
          geom_jitter(aes(color = as.factor(hospdead)), width = 0.1, height = 0.1) +
          scale_color_manual(values = c('0' = 'red', '1' = 'blue')) +
          labs(title = paste("Scatterplot of", var, "against hospdead status (Sampled)"),
               x = var, y = "Hospdead Status") +
          theme_minimal())
}

```
<details>


### Building a Logistic Regression Model

The development of our logistic regression model follows a systematic and iterative approach, emphasizing the importance of feature selection and model refinement. 

- **Initial Full Model**: We started by constructing an initial logistic regression model using all available variables. This provided a baseline understanding of the significance of each variable in predicting hospital mortality. Significant variables such as `log_charges`, `age`, `adlsc`, `scoma`, `bun`, `bili`, `slos`, `dzgroup`, and `income` were identified.

- **Iterative Model Building**: With the insights from the initial model, we began building the model from scratch, focusing on the most promising predictors. We started with `age`, given its strong initial significance.

- **Sequential Feature Inclusion**: We then progressively added variables such as `hday`, `scoma`, `adlsc`, and `num.co`, evaluating their impact on the model's performance at each step. This approach allowed us to understand the incremental value each feature added to the model.

- **Refinement and Simplification**: We observed that some variables, like `num.co` and `edu`, did not significantly improve the model and hence were removed. This step was crucial for maintaining model simplicity and avoiding overfitting.

- **Inclusion of Clinical Variables**: Clinical variables like `alb`, `bili`, `crea`, `bun`, `pafi`, and `meanbp` were then incorporated to explore their predictive value in the context of hospital mortality.

- **Disease Group and Demographics**: We also experimented with categorical variables such as `income` and `dzgroup`, assessing their impact on the prediction accuracy. The final model included the `dzgroup` variable due to its significant effect on patient outcomes.

- **Backward Stepwise Regression**: The final step involved applying backward stepwise regression to the expanded model. This helped in further simplifying the model by removing statistically insignificant variables, resulting in a more efficient and interpretable model.


<details>
<summary>Click for Code and Outputs </summary>
```{r}
logistic_full <- glm(hospdead ~ . , family = "binomial", data = final_df)
summary(logistic_full)


#The initial full model yields many significant terms, indicating variables of interest.
#like log_charges, age, adlsc, scoma, bun, bili, slos, dzgroup, income

#Therefore, we start building the model from scratch, beginning with the most promising #predictor: age.

# Logistic Regression Model Building and Selection

## Model 1: Initial Model
logistic_1 <- glm(hospdead ~ age , family = "binomial", data = final_df)
summary(logistic_1)

## Model 2: Adding 'hday'
# Adding 'hday' to the model, as time spent under critical care could be a strong indicator of patient health status.
logistic_2 <- update(logistic_1, . ~ . + hday)
summary(logistic_2)

## Model 3: Incorporating 'scoma'
#Incorporating 'scoma' (Activities of Daily Living Score) as it can be a strong indicator of patient health status.
logistic_3 <- update(logistic_2, . ~ . + scoma)
summary(logistic_3)

## Model 4: Adding 'adlsc'
#Adding 'adlsc' to explore if Daily living activity score correlates with mortality.
logistic_4 <- update(logistic_3, . ~ . + adlsc)
summary(logistic_4)

## Model 5: Including 'num.co'
#Including 'num.co' to assess the impact of the number of comorbidities on patient outcomes.
logistic_5 <- update(logistic_4, . ~ . + num.co)
summary(logistic_5)

## Model 6: Checking if 'log_charges' is significant
#Check if 'log_charges' is significant now
logistic_6 <- update(logistic_5, . ~ . + log_charges)
summary(logistic_6)

## Model 7: Removing 'num.co' due to redundancy
#Log_charges is highly insignificant, but num.co is insignificant,
#due to redundancy, so remove num.co from the model
logistic_6 <- update(logistic_6, . ~ . - num.co)
summary(logistic_6)

## Model 8: Expanding with clinical variables
#Expanding the model with clinical variables like 'alb', 'bili', 'crea', 'pafi', etc.
logistic_7 <- update(logistic_6, . ~ . + alb + bili + crea + bun + pafi + meanbp)
summary(logistic_7)

## Model 9: Expanding with 'income'
#Expanding the model with categorical variable 'income'
logistic_8 <- update(logistic_7, . ~ . + income)
summary(logistic_8)

## Model 10: Removing 'income' and adding 'dzgroup'
#Income turns out to be very mildly significant, so we remove it from the model and
#add 'dzgroup' (Disease Group) to assess the impact of disease type on patient outcomes.
logistic_9 <- update(logistic_8, . ~ . - income + dzgroup)
summary(logistic_9)

## Model 11: Checking if 'edu' is significant
#Check if 'edu' is significant now
logistic_10 <- update(logistic_9, . ~ . + edu)
summary(logistic_10)

## Model 12: Removing 'edu' and adding 'dementia'
#'Edu' is insignificant, so remove it from the model and check 'dementia'
logistic_10 <- update(logistic_10, . ~ . - edu + dementia)
summary(logistic_10)

## Model 13: Applying backward stepwise regression
#Applying backward stepwise regression to the expanded model to simplify it by removing statistically insignificant variables.
#This helps in achieving a simpler model that still captures essential predictors for hospital death.
logistic.simple <- step(logistic_10, direction = "backward")
summary(logistic.simple)
```
<details>


### Cross-Validation and Comparison: Logistic Regression Model Performance

**Initial Steps:**
1. **logistic_1 (Baseline with 'age')**: 
   - **Accuracy**: 0.743. 
   - **Observations**: A starting point, indicating room for improvement.

2. **logistic_2 (Addition of 'hday')**: 
   - **Accuracy**: 0.744, **Kappa**: 0.069.
   - **Observations**: Marginal improvement, showing the slight impact of the 'hday' feature.

**Incorporating Health Status:**
3. **logistic_3 (Addition of 'scoma')**: 
   - **Accuracy**: 0.781, **Kappa**: 0.295.
   - **Observations**: Significant improvement, highlighting the importance of health status indicators.

4. **logistic_4 (Introduction of 'adlsc')**: 
   - **Accuracy**: 0.782, **Kappa**: 0.301.
   - **Observations**: Consistent performance, consolidating gains with the addition of 'adlsc'.

**Clinical and Demographic Expansion:**
5. **logistic_6-8**: 
   - **Accuracy**: Around 0.775, **Kappa**: Modest gains.
   - **Observations**: Stability with further tuning using clinical and demographic variables, indicating the complex nature of these factors.

**Refined and Optimal Models:**
6. **logistic_9 (Inclusion of 'dzgroup')**: 
   - **Accuracy**: 0.783, **Kappa**: 0.35.
   - **Observations**: Peak performance, emphasizing the impact of disease grouping.

7. **logistic_10 (Testing 'edu' and 'dementia')**: 
   - **Accuracy**: 0.783, **Kappa**: 0.351.
   - **Observations**: Sustained peak performance, confirming the optimized combination of variables.

**Final Takeaways:**
- The evolution from logistic_1 to logistic_10 demonstrates a careful balance in adding predictive variables while monitoring for overfitting, as indicated by stable Accuracy.
- The top-performing models, logistic_9 and logistic_10, highlight the effectiveness of a comprehensive approach that blends demographic, clinical, and disease-specific predictors.
- This step-wise development process culminates in models that are not only accurate but also offer meaningful insights into the factors affecting patient outcomes.


<details>
<summary>Click for Code and Outputs </summary>
```{r}

cross_validate_model <- function(model, full_df, number_folds = 10) {

  # Extract the formula from the model
  model_formula <- formula(model)

  # Set up cross-validation control
  train_control <- trainControl(method = "cv", number = number_folds)

  # Train the model using cross-validation
  cv_model <- train(model_formula, data = full_df, method = "glm", trControl = train_control)

  # Return the results
  return(cv_model$results)
}

# Perform cross-validation for each of the remaining models
results1 <- cross_validate_model(logistic_1, full_df)
results2 <- cross_validate_model(logistic_2, full_df)
results3 <- cross_validate_model(logistic_3, full_df)
results4 <- cross_validate_model(logistic_4, full_df)
results5 <- cross_validate_model(logistic_5, full_df)
results6 <- cross_validate_model(logistic_6, full_df)
results7 <- cross_validate_model(logistic_7, full_df)
results8 <- cross_validate_model(logistic_8, full_df)
results9 <- cross_validate_model(logistic_9, full_df)
results10 <- cross_validate_model(logistic_10, full_df)

# Print the results for each as a table
library(knitr)
library(DT)

# Create a summary table using the correct metrics from the cross-validation results
results_df <- data.frame(
  Model = c("logistic_1", "logistic_2", "logistic_3", "logistic_4",
            "logistic_5", "logistic_6", "logistic_7", "logistic_8", "logistic_9", "logistic_10"),
  Accuracy = round(c(results1$Accuracy, results2$Accuracy, results3$Accuracy, 
                     results4$Accuracy, results5$Accuracy, results6$Accuracy, results7$Accuracy, results8$Accuracy, results9$Accuracy, results10$Accuracy), 3),
  Kappa = round(c(results1$Kappa, results2$Kappa, results3$Kappa, 
                  results4$Kappa, results5$Kappa, results6$Kappa, results7$Kappa, results8$Kappa, results9$Kappa, results10$Kappa), 3),
  AccuracySD = round(c(results1$AccuracySD, results2$AccuracySD, results3$AccuracySD, 
                       results4$AccuracySD, results5$AccuracySD, results6$AccuracySD, results7$AccuracySD, results8$AccuracySD, results9$AccuracySD, results10$AccuracySD), 3)
)

# Display the summary table
datatable(results_df, caption = 'Logistic Regression models Cross-validation Results', options = list(pageLength = 10))
```
<details>


### Confusion Matrix and ROC Curve Analysis of Selected Model

In this section, we delve into the performance evaluation of our chosen model, `logistic_10`, using detailed analysis tools like the confusion matrix and ROC curve. This analysis follows our decision to select `logistic_10` as the best model based on cross-validation results.

#### Model Performance:
- **Accuracy (0.792)**: The model correctly predicts survival and non-survival in 79.2% of cases, indicating a high overall rate of correct predictions.
- **Kappa (0.3821)**: Indicates a moderate agreement beyond chance between prediction and actual outcomes.

#### Class-wise Performance:
- **Sensitivity (92.33%)**: Demonstrates the model's high ability to correctly identify survivors (True Positive Rate).
- **Specificity (41.32%)**: Shows a lower ability to correctly identify non-survivors (True Negative Rate), suggesting a potential bias towards predicting survival.
- **Positive Predictive Value (81.95%)**: When the model predicts survival, there is an 81.95% chance it is correct.
- **Negative Predictive Value (65.11%)**: When the model predicts non-survival, there is a 65.11% chance it is correct.

#### Statistical Tests:
- **P-Value [Acc > NIR]**: Less than 2.2e-16, indicating that the model is significantly better than the No Information Rate.
- **Mcnemar's Test P-Value**: Less than 2.2e-16, suggesting a significant difference between type I and type II errors.

#### Overall Inferences:
- The model shows a more effective performance at identifying survivors than non-survivors.
- The imbalance in sensitivity and specificity suggests a potential bias towards predicting survival.
- The high prevalence rate (74.26%) could influence the predictive values, skewing them towards the more frequent class.
- The Balanced Accuracy (66.82%) points out the need for improvement in managing class imbalance.


<details>
<summary>Click for Code and Outputs </summary>
```{r}
# Load the required libraries
library(caret)

# Make predictions on the dataset
predictions <- predict(logistic_10, newdata = final_df, type = "response")
predicted_class <- ifelse(predictions > 0.5, 1, 0)  # Assuming 0.5 as threshold

# Generate the confusion matrix
conf_matrix0.5 <- confusionMatrix(as.factor(predicted_class), as.factor(final_df$hospdead))
print(conf_matrix0.5)
```
<details>

### ROC Curve and AUC Analysis

#### Interpretation for Hospdead Variable

- **AUC Analysis**: The AUC (Area Under the Curve) of 82.075% indicates that the model has a strong ability to distinguish between patients who died in the hospital (`hospdead = 1`) and those who did not (`hospdead = 0`).

- **Sensitivity and Specificity**: The ROC (Receiver Operating Characteristic) curve analysis highlights the trade-off between the True Positive Rate (TPR or sensitivity) and the False Positive Rate (FPR or 1 - specificity). Sensitivity measures the model's accuracy in correctly identifying actual in-hospital deaths, while specificity assesses how well the model avoids false alarms.

- **Impact of Class Distribution**: The prevalence of survivors in the dataset (`hospdead = 0`) can influence the model's performance, particularly its specificity. This underscores the importance of considering class distribution when evaluating model performance.

- **Threshold Strategy**: Depending on the clinical context, different thresholds may be preferred. In scenarios where missing a death is critical, a lower threshold can ensure higher sensitivity. Conversely, to reduce false alarms, a higher threshold might be chosen to improve specificity.

#### Final Analysis and Interpretation of Model's Performance

- **Overall Accuracy and Kappa**: With an accuracy of 79.2% and a moderate Kappa of 0.3813, the model demonstrates a reasonable balance between sensitivity and specificity, indicating useful predictive power.

- **Bias and Class Imbalance**: The model exhibits a potential bias towards predicting non-deaths, which could be due to the higher prevalence of this class in the dataset.

- **Conclusions and Recommendations**:
  - The high sensitivity is beneficial in critical scenarios where missing a non-death is crucial.
  - To improve specificity, consideration should be given to methods like dataset rebalancing or revising feature selection.
  - The moderate Kappa and significant Mcnemar's Test suggest further tuning of the model is necessary to achieve a better balance between sensitivity and specificity.


<details>
<summary>Click for Code and Outputs </summary>
```{r }
# Load the pROC library
library(pROC)

# Calculate the ROC curve
roc_response <- roc(response = final_df$hospdead, predictor = as.numeric(predictions), percent = TRUE)

# Specify the thresholds you want to display on the plot
thresholds_to_print <- c(0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9)

# Plotting the ROC curve with thresholds
plot(roc_response, 
     main = "ROC Curve Analysis", 
     xlab = "False Positive Rate (1 - Specificity)", 
     ylab = "True Positive Rate (Sensitivity)", 
     col = "#1c61b6", 
     lwd = 2,
     print.thres = thresholds_to_print, 
     print.thres.pattern = "%.2f", # Adding percent sign to thresholds
     print.thres.cex = 0.6,
     print.auc = TRUE,
     print.auc.pattern = "AUC: %.3f", # Customize AUC print format
     print.auc.col = "blue",
     print.auc.cex = 1.2,
     grid = TRUE,
     identity = TRUE, # Turn off the identity line
     max.auc.polygon = TRUE, # Do not fill the area up to the maximum AUC possible
     auc.polygon = TRUE,
     auc.polygon.col = "lightblue"
)
```
<details>



## Poisson Regression Analysis (GLM: Poisson)

### Research Context and Question

The focus of this analysis is on understanding the influence of various patient characteristics and hospital variables on the number of comorbidities (num.co) a patient has. This research question is critical in shedding light on the multifaceted aspects that contribute to the health complexity of patients.

### Exploratory Data Analysis: Poisson Distribution of `num.co`

- **Visualizing Distribution**: To gain insights into the distribution of the number of comorbidities (`num.co`), a histogram is utilized. 

- **Observations from Visualization**: The histogram indicates a right-skewed Poisson distribution for `num.co`. This skewness, characterized by a long tail to the right, suggests that while most patients have a lower number of comorbidities, there is a significant subset with a much higher number. 

- **Implications for Model Choice**: The observed Poisson distribution justifies the selection of Poisson regression for the analysis. Poisson regression is well-suited for count data, especially when the variance is close to the mean, which is typical in Poisson distributions.


<details>
<summary>Click for Code and Outputs </summary>
```{r}

# Load necessary library
library(ggplot2)

# Histogram to visualize the distribution of num.co with ticks incremented by 1
ggplot(final_df, aes(x = num.co)) +
  geom_histogram(bins = 30, fill = "skyblue", color = "black") +
  scale_x_continuous(breaks = seq(min(final_df$num.co), max(final_df$num.co), by = 1)) +
  labs(title = "Histogram of Number of Comorbidities (num.co)",
       x = "Number of Comorbidities",
       y = "Frequency") +
  theme_minimal()

```
<details>


### Poisson Regression: Model Building and Selection

#### Model Building Approach

The development of the Poisson regression model followed a systematic and iterative approach, emphasizing the significance and impact of each variable on model fit. This process is marked by careful evaluation, ensuring that each variable added to the model contributes meaningfully and interacts appropriately with others.

#### Key Steps in Model Development

1. **Initial Model with All Variables**: The process began with an all-inclusive model to assess the initial significance of each variable.

2. **Incremental Variable Addition**:
   - The model building commenced with 'adlsc' due to its high statistical significance.
   - Subsequent variables were added based on their significance and impact, including 'age', 'bun', 'temp', 'slos', 'dzgroup', 'dementia', and 'edu'.
   - At each step, variables that lost significance or had marginal impact were removed, such as 'crea', 'alb', 'bili', 'pafi', 'charges', 'scoma', and 'hospdead'.

3. **Evaluation and Refinement**: After each addition or removal, the model was re-evaluated for the significance of coefficients and overall model fit.

#### Final Model Composition

- The final model included significant variables: 'adlsc', 'age', 'bun', 'temp', 'slos', and various categories within 'dzgroup', along with 'dementia' and 'edu'. 
- The coefficients of these variables provided insights into their relationship with the response variable.

#### Interpretation of Model Coefficients

- Each coefficient in the final model represents the log-relative change in the response variable for a one-unit change in the predictor, holding all other variables constant.
- The significance levels of these coefficients indicate the robustness of their association with the response variable.


<details>
<summary>Click for Code and Outputs </summary>
```{r}
# Building an initial Poisson regression model using all variables
# to evaluate their initial significance.
poisson_model <- glm(num.co ~ ., family = "poisson", data = final_df)

# Summary of the model
summary(poisson_model)

# Model 1: Initial Model

step1_poisson <- glm(num.co ~ adlsc, family = "poisson", data = final_df)
summary(step1_poisson)

# Model 2: Adding 'age'

step2_poisson <- update(step1_poisson, . ~ . + age)
summary(step2_poisson)

# Model 3: Adding 'bun' and 'crea'

step3_poisson <- update(step2_poisson, . ~ . + bun + crea)
summary(step3_poisson)
# crea is insignificant, so remove it from the model
step3_poisson <- update(step3_poisson, . ~ . - crea)

# Model 4: Adding 'charges'

step4_poisson <- update(step3_poisson, . ~ . + charges)
summary(step4_poisson)

# Model 5: Adding 'temp', 'bili' and 'alb'

step5_poisson <- update(step4_poisson, . ~ . + temp + bili + alb)
summary(step5_poisson)

# Model 6: Adding 'slos'

step6_poisson <- update(step5_poisson, . ~ . + slos)
summary(step6_poisson)

# After adding 'slos', 'alb' becomes insignificant, so remove it from the model 
step6_poisson <- update(step6_poisson, . ~ . - alb)
summary(step6_poisson)

# We remove 'bili' consequent to the removal of 'alb' as it is marginally insignificant now, but add 'pafi' as it is significant

step6_poisson <- update(step6_poisson, . ~ . - bili + pafi)
summary(step6_poisson)

# Start adding categorical variables starting with 'dzgroup' and 'dementia'

step7_poisson <- update(step6_poisson, . ~ . + dzgroup + dementia)
summary(step7_poisson)

# 'pafi' and 'income' are insignificant now, so remove them from the model

step7_poisson <- update(step7_poisson, . ~ . - pafi - charges)
summary(step7_poisson)

# Check 'edu' and 'scoma' for significance

step8_poisson <- update(step7_poisson, . ~ . + edu + scoma)
summary(step8_poisson)

# 'scoma' is insignificant, so remove it from the model

step8_poisson <- update(step8_poisson, . ~ . - scoma)
summary(step8_poisson)

# Add 'hospdead' to the model

step9_poisson <- update(step8_poisson, . ~ . + hospdead)
summary(step9_poisson)

# 'hospdead' is insignificant, so remove it from the model

step9_poisson <- update(step9_poisson, . ~ . - hospdead)
summary(step9_poisson)

```
<details>

### Cross-Validation and Comparison: Poisson Regression Model Performance

In this phase, we utilized cross-validation to evaluate and compare various Poisson regression models. This approach allowed us to robustly assess the performance of each model iteration across different segments of the data.

**Initial Model Iterations (Steps 1-6):** 
- Models from step1 to step6 demonstrated a gradual improvement. For instance, the RMSE (Root Mean Square Error) decreased from 1.332 in step1_poisson to 1.308 in step6_poisson, indicating more accurate predictions.
- The R-squared value, a measure of how well the model explains the variability of the response data, saw a modest increase from 0.021 to 0.057 across these iterations.
- MAE (Mean Absolute Error) also improved slightly, from 1.055 in the first model to 1.036 in the sixth, suggesting enhanced prediction accuracy.

**Significant Improvement in Later Stages (Steps 7-9):**
- A notable jump in model performance was observed in steps 7 to 9. For example, the RMSE significantly dropped to 1.165 in step9_poisson from 1.308 in step6_poisson.
- The R-squared value experienced a substantial increase, reaching 0.251 in step9_poisson, compared to just 0.057 in step6_poisson. This leap indicates a considerably better fit of the model to the data.
- Similarly, MAE decreased to 0.913 in step9_poisson from 1.036 in step6_poisson, further validating the model's enhanced predictive accuracy.

**Optimal Model Identification:**
- The `step7_poisson` model emerged as the optimal choice. It struck a balance between model complexity and prediction accuracy, boasting a notable R-squared improvement and lower values of RMSE (1.168) and MAE (0.915).
- This model effectively predicts the number of comorbidities, highlighting the relevance of the variables included, especially the introduction of categorical variables in later steps that significantly improved model performance.
- The model avoids overfitting, as evidenced by its consistent performance across the cross-validation folds.


<details>
<summary>Click for Code and Outputs </summary>
```{r}

# Create a function for cross-validation
library(caret)

cross_validate_poisson <- function(model_formula, full_df, number_folds = 10) {
  # Set up cross-validation control
  train_control <- trainControl(method = "cv", number = number_folds, classProbs = FALSE, summaryFunction = defaultSummary)

  # Train the model using cross-validation
  cv_model <- train(model_formula, data = full_df, method = "glm", family = "poisson", trControl = train_control)
  
  # Return the results
  return(cv_model$results)
}

# Perform cross-validation for each of the remaining models
# Set a seed for reproducibility
set.seed(123)

# Perform cross-validation for each model step
results_step1 <- cross_validate_model(step1_poisson, final_df)
results_step2 <- cross_validate_model(step2_poisson, final_df)
results_step3 <- cross_validate_model(step3_poisson, final_df)
results_step4 <- cross_validate_model(step4_poisson, final_df)
results_step5 <- cross_validate_model(step5_poisson, final_df)
results_step6 <- cross_validate_model(step6_poisson, final_df)
results_step7 <- cross_validate_model(step7_poisson, final_df)
results_step8 <- cross_validate_model(step8_poisson, final_df)
results_step9 <- cross_validate_model(step9_poisson, final_df)

# Rounding and creating a data frame for the results
results_df <- data.frame(
  Model = c("step1_poisson", "step2_poisson", "step3_poisson", 
            "step4_poisson", "step5_poisson", "step6_poisson", 
            "step7_poisson", "step8_poisson", "step9_poisson"),
  RMSE = round(c(results_step1$RMSE, results_step2$RMSE, results_step3$RMSE, results_step4$RMSE, 
                results_step5$RMSE, results_step6$RMSE, results_step7$RMSE, results_step8$RMSE, results_step9$RMSE), 3),
  Rsquared = round(c(results_step1$Rsquared, results_step2$Rsquared, results_step3$Rsquared, results_step4$Rsquared, 
                    results_step5$Rsquared, results_step6$Rsquared, results_step7$Rsquared, results_step8$Rsquared, results_step9$Rsquared), 3),
  MAE = round(c(results_step1$MAE, results_step2$MAE, results_step3$MAE, results_step4$MAE, 
               results_step5$MAE, results_step6$MAE, results_step7$MAE, results_step8$MAE, results_step9$MAE), 3)
)

# Displaying the results in a datatable
datatable(results_df, caption = 'Poisson Model Cross-validation Results', options = list(pageLength = 10))

```
<details>





















