---
title: "Exploring Factors Affecting Critically Ill Patients' Condition through Machine Learning Methods"
author:
  - Samuel Devdas
  - Pradip Ravichandran
date: "2024-01-12"
output:
  prettydoc::html_pretty:
    theme: cayman
    css: styles.css
    highlight: github
    df_print: paged
    toc: true
    number_sections: true
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, results = 'markup')
```

## Abstract

This report details a comprehensive study conducted by Pradip Ravichandran and Samuel Devdas, focusing on the application of machine learning in healthcare economics and patient care. The study utilizes a dataset comprising 9105 critically ill patients from five U.S. medical centers, recorded between 1989 and 1994. The dataset, accessible at [Support2 Dataset](https://archive.ics.uci.edu/dataset/880/support2), forms the foundation for developing machine learning models aimed at predicting medical charges, the probability of death, and the number of comorbidities for patients on life support.

## Introduction

### Background and Significance

The significance of this research lies in its broad application across various critical domains, including:

1. **Financial Management:** The study seeks to improve budgeting and resource allocation in healthcare by employing predictive analytics for more efficient financial planning.
2. **Insurance Processing:** These models are designed to forecast potential expenses, offering vital insights for patients in managing insurance and out-of-pocket costs.
3. **Cost-Effectiveness:** An analysis focusing on the sustainability and practicality of life support treatments to aid in informed healthcare decision-making.
4. **Policy and Research Implications:** The outcomes of this research are anticipated to inform healthcare policy and economic research, thereby influencing the standard and accessibility of medical care.
5. **Ethical and Patient-Centric Considerations:** A key goal is to balance fiscal concerns with ethical considerations and patient-focused care in life support scenarios.

### Research Aims

The project stands at the critical intersection of machine learning, healthcare economics, and patient care, with the primary aim of providing insights for stakeholders in the healthcare sector.


## Data Description and Inspection

According to the HBiostat Repository (<https://hbiostat.org/data/repo/supportdesc>, Professor Frank Harrell) the following default values have been found to be useful in imputing missing baseline physiologic data:

| Baseline Variable   | short | Default Value |
|---------------------|-------|---------------|
| Serum albumin       | alb   | 3.5           |
| PaO2/FiO2 ratio     | pafi  | 333.3         |
| Bilirubin           | bili  | 1.01          |
| Serum Creatinine    | crea  | 1.01          |
| Blood urea nitrogen | bun   | 6.51          |


And following predictors should be removed, as they are findings from a previous model:

| Baseline Variable                             | short  |
|-----------------------------------------------|--------|
| APACHE III day 3 physiology score             | aps    |
| Patient had DNR order                         | dnr    |
| Day of DNR order (<0 if before study)         | dnrday |
| Physician’s 2-month survival estimate for pt. | prg2m  |
| Physician’s 6-month survival estimate for pt. | prg6m  |
| SUPPORT day 3 physiology score                | sps    |
| SUPPORT model 2-month survival estimate       | surv2m |
| SUPPORT model 6-month survival estimate       | surv6m |


## Dataset Preparation for Predictive Modeling

### Data Refinement Process

To ensure the efficacy of the predictive models, a rigorous dataset refinement process was undertaken. This process involved the following key steps:

1. **Removing Post-Admission Metrics**: 
   - **Rationale**: The study's focus is on initial patient assessments; therefore, variables measured post-admission were deemed irrelevant.
   - **Variables Excluded**: 'aps', 'dnr', 'dnrday', and 'surv2m'.

2. **Omitting Variables with Limited Predictive Value**: 
   - **Rationale**: Some variables were found to offer limited predictive value or were redundant.
   - **Variables Removed**: 'sod' (Sodium levels), 'ca' (Cancer), and 'urine'.

3. **Discarding Redundant or Low-Impact Variables**: 
   - **Rationale**: The goal was to streamline the dataset by eliminating variables that had minimal impact on the predictive models or were redundant.
   - **Variables Eliminated**: 'diabetes', 'dzclass', 'hrt', 'd.time', 'death', and 'resp'.

### Resulting Dataset

After this refinement process, the dataset was narrowed down to 20 key variables. This optimization aimed at enhancing the dataset's suitability for effective predictive modeling, thereby improving the accuracy and relevance of the study's findings.

<details>
<summary>Click for Code and Outputs </summary>
```{r}
# Load dataset
df_support <- read.csv("support2.csv", header = TRUE, sep = ",")

# Check the Variables' descriptions
df_description <- read.csv("support-variables-description.csv", header = TRUE, sep = ",")
df_description

# Inspect dataset
head(df_support, 3)

# Check structure of the dataset
str(df_support)
dim(df_support)

# Subset the data frame
selected_vars <- c("age", "hospdead", "slos", "dzgroup", "num.co", 
                   "edu", "income", "scoma", "charges", "hday", 
                   "dementia", "meanbp", "temp", "pafi", "alb", 
                   "bili", "crea", "ph", "bun", "adlsc")

# Create a new data frame with only the selected variables
df_support <- df_support[selected_vars]
```
<details>



## Data Cleaning and Preprocessing

### Overview

Data cleaning and preprocessing are critical steps in preparing a dataset for predictive modeling. In this phase, we focused on converting data into a format that can be effectively utilized in machine learning models, with particular attention to correctly formatting and processing categorical variables.

### Process Description

#### Categorical Variables Inspection

Our initial step involved a detailed inspection of the dataset, particularly focusing on categorical variables. We identified issues with data types of some variables. For instance, the `income` variable was incorrectly classified as a character (`chr`) type, which should have been a categorical (`factor`) type. This misclassification can impact the model's performance as it treats these variables inappropriately. 

- Example of Incorrect Data Type: `income: chr "$11-$25k" "$11-$25k" "under $11k"`

#### Code Implementation

1. **Identification of Categorical Variables:**
   We began by grouping the variables of interest (`hospdead`, `dzgroup`, `income`, `dementia`) which are categorical in nature.

2. **Verification of Unique Values:**
   Using the `sapply` function, we checked for unique values in these variables within the dataset to understand their structure and variability.

3. **Conversion to Factors:**
   To correct the data types, we converted these variables into factors. Notably, `income` was converted to an ordered factor to maintain the inherent order in income categories. This conversion is crucial for models that are sensitive to the ordinal nature of the data.

4. **Final Structure Validation:**
   After the conversion, we rechecked the structure of the dataset to ensure that the changes were correctly implemented.

This preprocessing step is essential to ensure that the subsequent analysis and model building are based on correctly formatted and meaningful data. Correctly classified categorical variables allow machine learning models to accurately interpret and utilize the data for predictions.



<details>
<summary>Click for Code and Outputs </summary>
```{r}
# Identify and Group the categorical variables as categories of interest
cat_interest <- c("hospdead", "dzgroup", "income", "dementia")

# Apply unique function to validate that the cat_interest have unique categorical values and check their structure
unique_values <- sapply(df_support[cat_interest], unique)
str(unique_values)

# Convert categorical and ordinal variables to factors
df_support$hospdead <- factor(df_support$hospdead)
df_support$dzgroup <- factor(df_support$dzgroup)
df_support$income <- factor(df_support$income, ordered = TRUE, 
                            levels = c("under $11k", "$11-$25k", "$25-$50k", ">$50k"))
df_support$dementia <- factor(df_support$dementia)

# Check structure of dataset to confirm changes
str(df_support[cat_interest])

```
<details>


## Imputing Missing Values

## Imputing Missing Values in Data Preprocessing

### Introduction

Handling missing data is a crucial step in the machine learning pipeline. Inconsistent handling of missing values can introduce bias or distort the underlying patterns in the data, impacting model performance.

### Methodology

1. **Predefined Imputation**: For certain variables (`alb`, `pafi`, `bili`, `crea`, `bun`), we impute missing values with predefined constants. This approach is often used when domain knowledge suggests a standard value or when a typical value is known. 

2. **Mean Imputation for 'ph'**: We impute the variable `ph` with the mean of its non-missing values. Mean imputation is effective for normally distributed data or when the proportion of missing data is low.

3. **Mode Imputation for Categorical Variables**: For categorical variables (`income`, `edu`), we employ mode imputation. This method is suitable for categorical data, substituting missing values with the most frequently occurring category.

4. **Validation of Imputation**: Post-imputation, we verify the absence of missing values in our dataset. This step ensures data integrity and completeness.

5. **Data Cleaning**: Finally, we remove any remaining rows with missing values. This step is vital in scenarios where imputation is not feasible or when the remaining missing data is minimal and unlikely to impact the dataset's overall quality.


<details>
<summary>Click for Code and Outputs </summary>
```{r}
# Impute missing values with specified values
impute_values <- c(
    alb = 3.5, pafi = 333.3, bili = 1.01,
    crea = 1.01, bun = 6.51
)

# Loop through the vector to impute values
for (var in names(impute_values)) {
    df_support[[var]][is.na(df_support[[var]])] <- impute_values[var]
}

# Impute the variable "ph" with the mean value
mean_ph <- mean(df_support$ph, na.rm = TRUE)
df_support$ph[is.na(df_support$ph)] <- mean_ph

# Impute the variable "income" with the mode value
getMode <- function(v) {
    uniqv <- unique(na.omit(v))
    uniqv[which.max(tabulate(match(v, uniqv)))]
}
mode_value <- getMode(df_support$income)
df_support$income[is.na(df_support$income)] <- mode_value

# Impute the variable "edu" with the mode value
mode_value <- getMode(df_support$edu)
df_support$edu[is.na(df_support$edu)] <- mode_value

# Check for Missing Values in df_support and sort in descending order
na_counts_col <- colSums(is.na(df_support))
na_counts_col <- sort(na_counts_col, decreasing = TRUE)
na_counts_col[1:15]

# Drop all the remaining rows with missing values.
final_df <- df_support[complete.cases(df_support), ]
dim(final_df)
sort(colnames(final_df))
```
<details>

## Exploratory Data Analysis

### Distribution Analysis of 'Charges'

#### Initial Observation
- The initial analysis of the `charges` variable reveals a highly right-skewed distribution. This skewness indicates that the majority of charges are less than 100,000, suggesting a concentration of lower values in our dataset.

#### Transformation Strategy
- To address the skewness and linearize the relationships, we apply a logarithmic transformation to the `charges` variable. This approach is standard in data analysis to handle skewed data distributions.

#### Implementation Details
1. **Logarithmic Transformation**:
   - The logarithm of zero or negative numbers is undefined. To circumvent this issue, we add a small constant (1 in this case) to the `charges` before applying the logarithmic transformation. This step ensures that all values are positive and suitable for the transformation.
   - `final_df$log_charges <- log(final_df$charges + 1)`

2. **Re-evaluation of Distribution**:
   - Post-transformation, the distribution of `log_charges` is examined. The histogram of log-transformed charges indicates a shift towards a more normal distribution. This transformation makes the data more suitable for various statistical and machine learning models that assume normality of features.


<details>
<summary>Click for Code and Outputs </summary>
```{r}
hist(final_df$charges, breaks = 50, main = "Histogram of Charges", xlab = "Charges", col = "blue", border = "black")

# The "charges" variable is highly right skewed with majority <100000

# To linearize the relationships, we take the logarithm of the 'charges' variable. However, since the log of zero or negative numbers is undefined, we add a small constant before taking the log to avoid errors. We then check the distribution of the resulting log-transformed charges.

# Taking the logarithm of 'charges' to linearize relationships
final_df$log_charges <- log(final_df$charges + 1)

# Plotting the histogram to check the distribution of log-transformed 'charges'
hist(final_df$log_charges, breaks = 50, 
     main = "Histogram of log_Charges", 
     xlab = "Charges", col = "blue", border = "black")
```


# Data Visualization: Exploring Relationships in Healthcare Data

## Objective

The primary aim here is to visually explore the relationships between various numeric variables and `log_charges`, a logarithmic transformation of medical charges, in a healthcare dataset. This examination is pivotal in identifying potential predictors for healthcare costs.

## Methodology

### Preparing Data for Plotting

- **Selection of Numeric Variables**: We began by isolating numeric variables from the dataset. These variables represent different quantitative aspects of healthcare data.

- **Excluding Target Variables**: From this numeric subset, we intentionally excluded  'charges'. 'log_charges' is our variable of interest, against which we plot other variables, and 'charges' is its non-logarithmic counterpart.

## Key Observations

- **Variables such as 'slos', 'edu', 'scoma', 'hday', 'hrt', 'temp', 'bili', 'crea', and 'bun' showed a positive relationship with 'log_charges'**. This indicates that as these variables increase, there's a tendency for the logarithm of medical charges to increase as well.

- **Variables like 'age', 'num.co', and 'pafi' exhibited a negative relationship with 'log_charges'**. This suggests that higher values of these variables are associated with lower logarithmic medical charges.


<details>
<summary>Click for Code and Outputs </summary>
```{r}
# Make data ready for plotting
numeric_vars <- names(final_df)[sapply(final_df, is.numeric)]
numeric_vars

# Exclude 'log_charges' from Numeric Columns
numeric_vars <- numeric_vars[!(numeric_vars %in% c("log_charges", "charges"))]
numeric_vars

# Loop through each numeric variable and create a scatterplot against 'log_charges'
library(ggplot2)
for (var in numeric_vars) {
  print(
    ggplot(final_df, aes_string(x = var, y = "log_charges")) +
      geom_point(shape = 19, size = 1, alpha = 0.5, color = "blue") +
      geom_smooth(method = "lm", se = FALSE, color = "red") +  # Add a linear regression line
      labs(title = paste("Scatterplot of log_charges vs", var), x = var, y = "log_charges")
  )
}
```
<details>


## Predicting Charges for Life Support Patients: A Critical Aspect of Healthcare Management

### Identifying Key Variables Influencing Life Support Patient Charges

#### Overview

In this section, we discuss the development of multiple linear models to identify the most significant variables impacting the 'log_charges' for life support patients. This is a crucial aspect of healthcare management, as understanding these variables can aid in cost prediction and management.

#### Model Development Process

1. **Initial Model Creation**: We start by fitting a linear model (`lm.charges1`) using all available variables except for 'charges'. This model serves as a baseline to identify potential predictors of 'log_charges'.

2. **Iterative Model Refinement**: 
   - We employ a stepwise approach, systematically removing variables that show the least significance based on their p-values and F-test results. This process involves updating the model multiple times (from `lm.charges1` to `lm.charges8`) to refine its predictive accuracy.
   - In each step, variables like 'crea', 'adlsc', 'alb', 'pafi', and 'scoma', among others, are excluded based on statistical tests (`drop1` analysis). 
   - The aim is to simplify the model by keeping only those variables that significantly contribute to the prediction of 'log_charges'.

3. **Assessing Model Fit**:
   - Throughout this process, we continuously assess the model fit using summary statistics and Akaike Information Criterion (AIC). This helps in determining the balance between model complexity and explanatory power.
   - By the final model (`lm.charges8`), we reach a point where further simplification would lead to a loss of predictive power, indicating an optimal set of variables.

#### Insights and Decision-Making

- **Variable Importance**: This process reveals the most critical variables impacting 'log_charges'. By removing variables one by one and observing the changes in the model's performance, we identify those factors that have the most substantial influence.

- **Model Simplification**: The stepwise refinement underscores the importance of simplicity in model construction. By removing less significant variables, we enhance the model's interpretability and efficiency without compromising its predictive ability.

- **Methodological Considerations**: This iterative approach demonstrates a careful balance between statistical rigor and practical relevance. The decisions made at each step are grounded in statistical tests and the overarching goal of model optimization.

<details>
<summary>Click for Code and Outputs </summary>
```{r}
# Fitting the starting model using all the variables except 'charges'
# Select only numeric columns from the data frame
numeric_columns <- sapply(final_df, is.numeric)

# Create a new data frame without the 'charges' column
full_df <- subset(final_df, select = -charges) 

# Fit a linear model to predict log-transformed charges using all other variables
lm.charges1 <- lm(log_charges~ ., data = full_df)

# Display the summary of the model fit
summary(lm.charges1)

# Dropping the least significant variable from the model and testing the difference
drop1(lm.charges1, test = "F")

# Updating the model by excluding identified variables
lm.charges2 <- update(lm.charges1, . ~ . - crea)
summary(lm.charges2)

# Dropping variables with higher p-values and inspecting the AIC impact
drop1(lm.charges2, test = "F")

# Simplifying the model based on the drop1 analysis and keeping variables of interest
# Removing the variables adlsc, alb, pafi, and scoma from the model
lm.charges3 <- update(lm.charges2, . ~ . -adlsc -alb - pafi -scoma )
summary(lm.charges3)

# Dropping variables one by one from the model lm.charges3 and testing with F-test
drop1(lm.charges3, test = "F")

# Updating the model lm.charges3 by removing the variables 'ph', 'dementia', 'num.co', 'meanbp'
# and then summarizing the new model lm.charges4
lm.charges4 <- update(lm.charges3, . ~ . - ph - dementia - num.co - meanbp )
summary(lm.charges4)

# Testing the model by dropping one variable at a time
drop1(lm.charges4, test = "F")

# Updating the model by removing the 'slos' variable
lm.charges5 <- update(lm.charges4, . ~ . - slos )
summary(lm.charges5)

# Performing a drop1 test on the model lm.charges5 to check which variable can be dropped
drop1(lm.charges5, test = "F")

# Updating the model by dropping the 'income' variable due to its high p-value
lm.charges6 <- update(lm.charges5, . ~ . - income)
summary(lm.charges6)

# Conducting drop1 analysis to test individual model terms
drop1(lm.charges6, test = "F")

# Updating the model by dropping variables with higher p-values, in this case 'bili'
lm.charges7 <- update(lm.charges6, . ~ . - bili )
summary(lm.charges7)

# Conducting drop1 analysis to see the effect of dropping each variable on model fit
drop1(lm.charges7, test = "F")

# Updating the model by dropping the variable with the highest AIC impact
lm.charges8 <- update(lm.charges7, . ~ . - hday)
summary(lm.charges8)

# Conducting final drop1 analysis to check for any remaining insignificant variables.
drop1(lm.charges8, test = "F")

# Finally, the model cannot be further simplified without loss of predictive power.

```
<details>

###CROSS VALIDATION AND BEST MODEL SELECTION##########

Cross validating all the linear models to select the best model

<details>
<summary>Click for Code and Outputs </summary>
```{r}
# Load the required package
library(caret)

# Setting a seed for reproducibility
set.seed(123)

# Create a function for cross-validation
cross_validate_model <- function(model, full_df, number_folds = 10) {
  # Extract the formula from the model
  model_formula <- formula(model)
  
  # Set up cross-validation control
  train_control <- trainControl(method = "cv", number = number_folds)
  
  # Train the model using cross-validation
  cv_model <- train(model_formula, data = full_df, method = "lm", trControl = train_control)
  
  # Return the results
  return(cv_model$results)
}

# Perform cross-validation for each of the remaining models
results1 <- cross_validate_model(lm.charges1, full_df)
results2 <- cross_validate_model(lm.charges2, full_df)
results3 <- cross_validate_model(lm.charges3, full_df)
results4 <- cross_validate_model(lm.charges4, full_df)
results5 <- cross_validate_model(lm.charges5, full_df)
results6 <- cross_validate_model(lm.charges6, full_df)
results7 <- cross_validate_model(lm.charges7, full_df)
results8 <- cross_validate_model(lm.charges8, full_df)

# Load libraries for printing tables
library(knitr)
library(DT)

# Rounding the values in the data frame
results_df <- data.frame(
  Model = c("lm.charges1", "lm.charges2", "lm.charges3", "lm.charges4",
            "lm.charges5", "lm.charges6", "lm.charges7", "lm.charges8"),
  RMSE = round(c(results1$RMSE, results2$RMSE, results3$RMSE, 
                 results4$RMSE, results5$RMSE, results6$RMSE, results7$RMSE, results8$RMSE), 3),
  Rsquared = round(c(results1$Rsquared, results2$Rsquared, 
                     results3$Rsquared, results4$Rsquared, results5$Rsquared, results6$Rsquared, results7$Rsquared, results8$Rsquared), 3),
  MAE = round(c(results1$MAE, results2$MAE, results3$MAE, 
                results4$MAE, results5$MAE, results6$MAE, results7$MAE, results8$MAE), 3)
)

# Display the results as a table
datatable(results_df, caption = 'Linear Model Development Cross-validation Results', 
          options = list(pageLength = 10 ))
```
<details>

#### Linear Models Cross-validation Results Brief ########

lm.charges1 and lm.charges2: Best accuracy (RMSE ~0.726, R-squared ~0.676).
lm.charges3: Slightly lower performance.
lm.charges4: Decent trade-off (RMSE: 0.731, R-squared: 0.671).
lm.charges5: Lower accuracy (RMSE: 0.904, R-squared: 0.497), possible overfitting.
Conclusion: We select lm.charges4 for further development as it stands out for balancing simplicity and effectiveness.

### INTERACTION-TERMS EFFECT ON model 'lm.charges4'####

Are there any interesting interactions effects on charges due to
any variables?
1. Does the impact of age on healthcare charges vary significantly
across different disease groups in the dataset?


<details>
<summary>Click for Code and Outputs </summary>
```{r}
# Adding an interaction term between 'age' and 'dzgroup' to the model
lm_age_dzgroup <- lm(log_charges ~ age * dzgroup 
                     + hospdead + edu + income
                     + hday + temp + bili + bun, 
                     data = full_df)

# Checking the summary of the model to see the interaction effect
summary_lm_age_dzgroup <- summary(lm_age_dzgroup)
print(summary_lm_age_dzgroup)
```
<details>

Summary of Interaction Effects between Age and Disease Groups
- CHF: Significant decrease in charges with age.
- Coma: Significant decrease in charges with age.
- MOSF w/Malignancy: Significant decrease in charges with age.
- Cirrhosis, Colon Cancer, COPD, Lung Cancer: No significant interaction.

2. Is the effect of hospital death on healthcare charges modified
by the type of disease a patient has?
Adding an interaction term between hospital stay length (hospdead) and disease group

<details>
<summary>Click for Code and Outputs </summary>
```{r}
# Fit a linear model with interaction between hospdead and dzgroup, controlling for other variables
lm_hospdead_dzgroup <- lm(log_charges ~ hospdead * dzgroup 
                                      + age + edu + income + hday 
                                      + temp + bili + bun, 
                                      data = full_df)

# Check the summary of the model to see the interaction effect
summary_lm <- summary(lm_hospdead_dzgroup)
summary_lm
```
<details>

#### CROSS VALIDATE Linear Models with Interaction terms AND COMPARE RESULTS

Perform cross-validation for the Age-Disease Group & Hospdead-Disease Group Interaction Models

<details>
<summary>Click for Code and Outputs </summary>
```{r}
# Set a seed for reproducibility
set.seed(123)

# Perform cross-validation for the Age-Disease Group Interaction Model
results_age_dzgroup <- cross_validate_model(lm_age_dzgroup, full_df)

# Perform cross-validation for the Hospdead-Disease Group Interaction Model
results_hospdead_dzgroup <- cross_validate_model(lm_hospdead_dzgroup, full_df)

# Rounding and creating a data frame for the results
results_df2 <- data.frame(
  Model = c("lm.charges4", "lm_age_dzgroup", "lm_hospdead_dzgroup"),
  RMSE = round(c(results4$RMSE, results_age_dzgroup$RMSE, results_hospdead_dzgroup$RMSE), 3),
  Rsquared = round(c(results4$Rsquared, results_age_dzgroup$Rsquared, results_hospdead_dzgroup$Rsquared), 3),
  MAE = round(c(results4$MAE, results_age_dzgroup$MAE, results_hospdead_dzgroup$MAE), 3)
)

# Displaying the results in a datatable
datatable(results_df2, caption = 'Selected Interaction Cross-validation Results', options = list(pageLength = 5))
```
<details>


#### Linear Models with Interaction: Cross-validation Results Brief 

Built a Non-Linear model upon lm.charges4, lm_age_dzgroup and lm_hospdead_dzgroup,
with RMSEs over 0.890 and R-squared around 0.500, are less precise but offer
insights into specific age and disease group dynamics.

### Non Linear Models

#### Visual Exploration of Non-Linear Trends

Loop through each numeric variable to visually explore potential non-linear relationships

<details>
<summary>Click for Code and Outputs </summary>
```{r}
# Load the ggplot2 library for plotting
library(ggplot2)

# Loop through each numeric variable to create scatterplots with a loess smoother
for (var in numeric_vars) {
  # Creating a scatterplot with a smoother to detect non-linear trends
  print(
    ggplot(final_df, aes_string(x = var, y = "log_charges")) +
      geom_point(shape = 19, size = 1, alpha = 0.5, color = "blue") +
      geom_smooth(method = "loess", se = TRUE, color = "red", fill = "orange") +  # Loess smoother for non-linear trends
      labs(title = paste("Exploring Non-Linear Relationship of", var, "with log_charges"),
           x = var, y = "log_charges")
  )
}
```
<details>
  
Each plot shows the relationship between 'log_charges' and a numeric variable
The smoother (red line with orange confidence area) helps identify potential non-linear patterns

#### EDA Summary: Non-Linear Effects
From the graph we can see that there are some non-linear effects in the following variables:
Probable Quadratic Effects: meanbp, bun, ph,slos
Polynomial Effects: temp,edu, hrt, hday, bili, num.co, alb
No significant non-linear trend in age.

#### Iterating on the lm.charges4, adding non linear effects

Building a model incorporating observed non-linear effects

num.co - Higher order terms are insignificant, change to linear
bili - poly3 is insignificant, change to poly2
edu - higher order terms are insignificant, change to linear
meanbp - quadratic term is insignificant, change to linear

<details>
<summary>Click for Code and Outputs </summary>
```{r}
# Update the lm.charges4 model with non-linear terms
nonlinear.lm.charges4 <- update(lm.charges4, . ~ . -slos - meanbp - bun - ph - alb 
                                - temp - edu - hday - bili - num.co +
                                  poly(slos, 2) +
                                  poly(meanbp, 2) + 
                                  poly(bun, 2) +
                                  poly(ph, 2) +
                                  poly(alb, 2) +
                                  poly(temp, 3) + 
                                  poly(edu, 3) +
                                  poly(hday, 3) +
                                  poly(bili, 3) +
                                  poly(num.co, 3))

# Summary of the updated model
summary(nonlinear.lm.charges4)

```
<details>

#### Simplify model, remove poly terms for num.co, bili, meanbp

<details>
<summary>Click for Code and Outputs </summary>
```{r}
lm.nonlinear.trim <- update(nonlinear.lm.charges4, . ~ .  
                              - poly(num.co, 3) - poly(bili,3) - poly(meanbp, 2)
                              - poly(alb, 2) - poly(ph, 2)
                              )
summary(lm.nonlinear.trim)
```
<details>

### CROSS VALIDATE AND COMPARE RESULTS OF THE NON-LINEAR MODELS

Perform cross-validation for nonlinear.lm.charges4

<details>
<summary>Click for Code and Outputs </summary>
```{r}
# Perform cross-validation for nonlinear.lm.charges4
results_nonlinear_charges4 <- cross_validate_model(nonlinear.lm.charges4, full_df)

# Perform cross-validation for the final trimmed model
results_trimmed <- cross_validate_model(lm.nonlinear.trim, full_df)

# Combine the selected results into one data frame
selected_results_df <- data.frame(
  Model = c(
    "lm.charges4", "Nonlinear lm.charges4", "Trimmed Nonlinear Model"
  ),
  RMSE = round(c(
    results4$RMSE, results_nonlinear_charges4$RMSE, results_trimmed$RMSE
  ), 3),
  Rsquared = round(c(
    results4$Rsquared, results_nonlinear_charges4$Rsquared, results_trimmed$Rsquared
  ), 3),
  MAE = round(c(
    results4$MAE, results_nonlinear_charges4$MAE, results_trimmed$MAE
  ), 3)
)

# Display the selected results table
datatable(selected_results_df, caption = 'Selected Non-Linear Cross-validation Results Comparison', options = list(pageLength = 10))
```
<details>

### Cross-validation Results Brief ###

Nonlinear adaptations of lm.charges4 led to two distinct models: Nonlinear lm.charges4 and Trimmed Nonlinear Model.
Nonlinear lm.charges4, with an RMSE of 0.649 and R-squared of 0.741, shows marked improvement over the linear lm.charges4 (RMSE: 0.731, R-squared: 0.671).
The Trimmed Nonlinear Model, further refined with fewer variables, achieves a similar performance (RMSE: 0.65, R-squared: 0.742), suggesting effective optimization without loss of predictive ability.

Conclusion: The transition to nonlinear modeling significantly enhances the accuracy of the original lm.charges4, with the trimmed version balancing model complexity and predictive precision.

 
## GENERALIZED ADDITIVE MODEL
We use variables from our most performant model:"lm.nonlinear.trim",
to try to build a model that captures non-linear dynamics completely.


<details>
<summary>Click for Code and Outputs </summary>
```{r, echo=FALSE}
# Load GAM library
library(mgcv)

# Using the numeric variables from lm.nonlinear.trim to build a base GAM
gam.model1 <- gam(log_charges ~ s(age) +
                                s(slos) + 
                                s(bun) +
                                s(alb) +
                                s(temp) + 
                                s(hday) +
                                s(bili) +
                                s(edu) ,
                              data = full_df)

# Summary of the GAM model
summary(gam.model1)
```
<details>

### Cross-validate the GAM model 

<details>
<summary>Click for Code and Outputs </summary>
```{r}
# Load the 'mgcv' library
library(mgcv)

# Prepare Data for K-Fold Cross-validation
set.seed(123)  # for reproducibility
k <- 10  # number of folds
n <- nrow(full_df)
fold_ids <- sample(rep(1:k, length.out = n))

# Define a function to perform k-fold cross-validation
perform_cv_for_gam <- function(model, data, k) {
  # Initialize vectors to store performance metrics
  rmse_values <- numeric(k)
  r_squared_values <- numeric(k)

  # Generate fold IDs for cross-validation
  set.seed(123)  # For reproducibility
  fold_ids <- sample(rep(1:k, length.out = nrow(data)))

  # Perform k-fold cross-validation
  for (i in 1:k) {
    train_data <- data[fold_ids != i, ]
    test_data <- data[fold_ids == i, ]

    # Make predictions and calculate RMSE and R-squared
    predictions <- predict(model, test_data, type = "response")
    rmse_values[i] <- sqrt(mean((predictions - test_data$log_charges)^2))
    r_squared_values[i] <- cor(predictions, test_data$log_charges)^2
  }

  # Calculate and return the average RMSE and R-squared
  list(mean_rmse = mean(rmse_values), mean_r_squared = mean(r_squared_values))
}

# Results of 10-fold cross-validation
cv_results <- perform_cv_for_gam(gam.model1, full_df, 10)
gam_mean_rmse <- cv_results$mean_rmse
gam_mean_r_squared <- cv_results$mean_r_squared

# Combine the Selected Results into One Data Frame
selected_results_df <- data.frame(
  Model = c(
    "lm.charges4", "Nonlinear lm.charges4", "Trimmed Nonlinear Model", "GAM Model"
  ),
  RMSE = round(c(
    results4$RMSE, results_nonlinear_charges4$RMSE, results_trimmed$RMSE, gam_mean_rmse
  ), 3),
  Rsquared = round(c(
    results4$Rsquared, results_nonlinear_charges4$Rsquared, results_trimmed$Rsquared, gam_mean_r_squared
  ), 3),
  MAE = c(
    round(results4$MAE, 3), round(results_nonlinear_charges4$MAE, 3), round(results_trimmed$MAE, 3), NA
  )
)

# Display the selected results table
datatable(selected_results_df, caption = 'Selected GAM model Cross-validation Results Comparison', options = list(pageLength = 10))
```
<details>

### GAM: Cross-validation Results

Cross-validation Results Summary:

lm.charges4:
- RMSE: 0.731, R-squared: 0.671, MAE: 0.57
- Provides a basic level of predictive accuracy, serving as a baseline.

Nonlinear lm.charges4:
- RMSE: 0.649, R-squared: 0.741, MAE: 0.502
- Shows improved accuracy over the basic model, indicating the benefit of introducing nonlinearity.

Trimmed Nonlinear Model:
- RMSE: 0.647, R-squared: 0.742, MAE: 0.503
- Offers a slight improvement in accuracy with a simpler, more interpretable model compared to the Nonlinear lm.charges4.

GAM Model:
- RMSE: 0.659, R-squared: 0.732
- Despite its flexibility in capturing complex patterns, it performs better to the Nonlinear models, with fewer variables, which shows overfitting in the Nonlinear models.

Conclusion:
- The Trimmed Nonlinear Model emerges as the best choice, offering high accuracy with a simpler, more interpretable model structure.
- The GAM Model, has the best performance, but lacks interpretability like the Nonlinear models and linear models.without the categorical variables.


## GLM: Generalized Linear Models

What are the key predictors that significantly influence the likelihood of
a patient's death in the hospital?

<details>
<summary>Click for Code and Outputs </summary>
```{r, echo=FALSE}
# Make data ready for plotting
numeric_vars <- names(final_df)[sapply(final_df, is.numeric)]
numeric_vars

# Exclude 'log_charges' from Numeric Columns
numeric_vars <- numeric_vars[!(numeric_vars %in% c("charges"))]
numeric_vars

# Load the ggplot2 library
library(ggplot2)

# Set the sample size for better visibility of the scatterplots
sample_size <- 500  # Adjust this number based on your data size and needs

for (var in numeric_vars) {
  # Take a random sample of the data
  sampled_data <- final_df[sample(nrow(final_df), sample_size), ]
  
  # Create the scatterplot with the sampled data
  print(ggplot(sampled_data, aes_string(x = var, y = 'hospdead')) +
          geom_jitter(aes(color = as.factor(hospdead)), width = 0.1, height = 0.1) +
          scale_color_manual(values = c('0' = 'red', '1' = 'blue')) +
          labs(title = paste("Scatterplot of", var, "against hospdead status (Sampled)"),
               x = var, y = "Hospdead Status") +
          theme_minimal())
}

```
<details>


OBSERVATION: The visual inspection of scatterplots comparing each numeric
variable with the hospdead status reveals a few potential separation patterns in variables like:
log_charges, num.co, age, adlsc and alb.

### Building a Logistic Regression Model

Building an initial logistic regression model using all variables
to evaluate their initial significance.

<details>
<summary>Click for Code and Outputs </summary>
```{r}
logistic_full <- glm(hospdead ~ . , family = "binomial", data = final_df)
summary(logistic_full)
```
<details>

The initial full model yields many significant terms, indicating variables of interest.
like log_charges, age, adlsc, scoma, bun, bili, slos, dzgroup, income

Therefore, we start building the model from scratch, beginning with the most promising predictor: age.


<details>
<summary>Click for Code and Outputs </summary>
```{r combined-modeling, echo=FALSE}
# Logistic Regression Model Building and Selection

## Model 1: Initial Model
logistic_1 <- glm(hospdead ~ age , family = "binomial", data = final_df)
summary(logistic_1)

## Model 2: Adding 'hday'
# Adding 'hday' to the model, as time spent under critical care could be a strong indicator of patient health status.
logistic_2 <- update(logistic_1, . ~ . + hday)
summary(logistic_2)

## Model 3: Incorporating 'scoma'
#Incorporating 'scoma' (Activities of Daily Living Score) as it can be a strong indicator of patient health status.
logistic_3 <- update(logistic_2, . ~ . + scoma)
summary(logistic_3)

## Model 4: Adding 'adlsc'
#Adding 'adlsc' to explore if Daily living activity score correlates with mortality.
logistic_4 <- update(logistic_3, . ~ . + adlsc)
summary(logistic_4)

## Model 5: Including 'num.co'
#Including 'num.co' to assess the impact of the number of comorbidities on patient outcomes.
logistic_5 <- update(logistic_4, . ~ . + num.co)
summary(logistic_5)

## Model 6: Checking if 'log_charges' is significant
#Check if 'log_charges' is significant now
logistic_6 <- update(logistic_5, . ~ . + log_charges)
summary(logistic_6)

## Model 7: Removing 'num.co' due to redundancy
#Log_charges is highly insignificant, but num.co is insignificant,
#due to redundancy, so remove num.co from the model
logistic_6 <- update(logistic_6, . ~ . - num.co)
summary(logistic_6)

## Model 8: Expanding with clinical variables
#Expanding the model with clinical variables like 'alb', 'bili', 'crea', 'pafi', etc.
logistic_7 <- update(logistic_6, . ~ . + alb + bili + crea + bun + pafi + meanbp)
summary(logistic_7)

## Model 9: Expanding with 'income'
#Expanding the model with categorical variable 'income'
logistic_8 <- update(logistic_7, . ~ . + income)
summary(logistic_8)

## Model 10: Removing 'income' and adding 'dzgroup'
#Income turns out to be very mildly significant, so we remove it from the model and
#add 'dzgroup' (Disease Group) to assess the impact of disease type on patient outcomes.
logistic_9 <- update(logistic_8, . ~ . - income + dzgroup)
summary(logistic_9)

## Model 11: Checking if 'edu' is significant
#Check if 'edu' is significant now
logistic_10 <- update(logistic_9, . ~ . + edu)
summary(logistic_10)

## Model 12: Removing 'edu' and adding 'dementia'
#'Edu' is insignificant, so remove it from the model and check 'dementia'
logistic_10 <- update(logistic_10, . ~ . - edu + dementia)
summary(logistic_10)

## Model 13: Applying backward stepwise regression
#Applying backward stepwise regression to the expanded model to simplify it by removing statistically insignificant variables.
#This helps in achieving a simpler model that still captures essential predictors for hospital death.
logistic.simple <- step(logistic_10, direction = "backward")
summary(logistic.simple)
```
<details>


### CROSS VALIDATION LOGISTIC MODELS AND COMPARE RESULTS

Perform cross-validation for each of the logistic regression models

<details>
<summary>Click for Code and Outputs </summary>
```{r}

cross_validate_model <- function(model, full_df, number_folds = 10) {

  # Extract the formula from the model
  model_formula <- formula(model)

  # Set up cross-validation control
  train_control <- trainControl(method = "cv", number = number_folds)

  # Train the model using cross-validation
  cv_model <- train(model_formula, data = full_df, method = "glm", trControl = train_control)

  # Return the results
  return(cv_model$results)
}

# Perform cross-validation for each of the remaining models
results1 <- cross_validate_model(logistic_1, full_df)
results2 <- cross_validate_model(logistic_2, full_df)
results3 <- cross_validate_model(logistic_3, full_df)
results4 <- cross_validate_model(logistic_4, full_df)
results5 <- cross_validate_model(logistic_5, full_df)
results6 <- cross_validate_model(logistic_6, full_df)
results7 <- cross_validate_model(logistic_7, full_df)
results8 <- cross_validate_model(logistic_8, full_df)
results9 <- cross_validate_model(logistic_9, full_df)
results10 <- cross_validate_model(logistic_10, full_df)

# Print the results for each as a table
library(knitr)
library(DT)

# Create a summary table using the correct metrics from the cross-validation results
results_df <- data.frame(
  Model = c("logistic_1", "logistic_2", "logistic_3", "logistic_4",
            "logistic_5", "logistic_6", "logistic_7", "logistic_8", "logistic_9", "logistic_10"),
  Accuracy = round(c(results1$Accuracy, results2$Accuracy, results3$Accuracy, 
                     results4$Accuracy, results5$Accuracy, results6$Accuracy, results7$Accuracy, results8$Accuracy, results9$Accuracy, results10$Accuracy), 3),
  Kappa = round(c(results1$Kappa, results2$Kappa, results3$Kappa, 
                  results4$Kappa, results5$Kappa, results6$Kappa, results7$Kappa, results8$Kappa, results9$Kappa, results10$Kappa), 3),
  AccuracySD = round(c(results1$AccuracySD, results2$AccuracySD, results3$AccuracySD, 
                       results4$AccuracySD, results5$AccuracySD, results6$AccuracySD, results7$AccuracySD, results8$AccuracySD, results9$AccuracySD, results10$AccuracySD), 3)
)

# Display the summary table
datatable(results_df, caption = 'Logistic Regression models Cross-validation Results', options = list(pageLength = 10))
```
<details>

### Logistic regression Model Development and Performance Summary:

Initial Steps:
- logistic_1 (baseline with 'age'): Accuracy at 0.743; a starting point with room for improvement.
- logistic_2 (addition of 'hday'): Slight increase in Accuracy to 0.744 and Kappa to 0.069, indicating marginal improvement.

Incorporating Health Status:
- logistic_3 (addition of 'scoma'): Significant leap in Accuracy to 0.781 and Kappa to 0.295, showing the importance of health status indicators.
- logistic_4 (introduction of 'adlsc'): Maintains Accuracy at 0.782 and Kappa at 0.301, consolidating gains.

Clinical and Demographic Expansion:
- logistic_6-8: Further fine-tuning with clinical and demographic variables. Accuracy stabilizes around 0.775, and Kappa sees modest gains, highlighting the complex nature of these factors.

Refined and Optimal Models:
- logistic_9 (inclusion of 'dzgroup'): Peaks in Accuracy (0.783) and Kappa (0.35), underscoring the impact of disease grouping.
- logistic_10 (testing 'edu' and 'dementia'): Sustains peak Accuracy and Kappa (0.351), confirming the optimized combination of variables.

Final Takeaways:
- The evolution from logistic_1 to logistic_10 illustrates a careful balance of adding predictive variables while monitoring overfitting (as shown by stable AccuracySD).
- The best models, logistic_9 and logistic_10, demonstrate the effectiveness of a comprehensive approach, blending demographic, clinical, and disease-specific predictors.
- This step-wise development process resulted in models that not only predict accurately but also provide meaningful insights into the factors affecting patient outcomes

### CONFUSION MATRIX AND ROC CURVE ANALYSIS

Selecting logistic_10 as our best model based on cross-validation results
We want to deep dive understand the performance of the model using a confusion matrix and ROC curve.

<details>
<summary>Click for Code and Outputs </summary>
```{r}
# Load the required libraries
library(caret)

# Make predictions on the dataset
predictions <- predict(logistic_10, newdata = final_df, type = "response")
predicted_class <- ifelse(predictions > 0.5, 1, 0)  # Assuming 0.5 as threshold

# Generate the confusion matrix
conf_matrix0.5 <- confusionMatrix(as.factor(predicted_class), as.factor(final_df$hospdead))
print(conf_matrix0.5)
```
<details>

#### Confusion Matrix (threshold = 0.5) Interpretation:

Model Performance:
- Accuracy: 0.792. Model correctly predicts survival and non-survival in 79.2% of cases.
- Kappa: 0.3813. Moderate agreement beyond chance between prediction and actual outcomes.

Class-wise Performance:
- Sensitivity: 92.34%. High ability to correctly identify survivors (True Positive Rate).
- Specificity: 41.22%. Lower ability to correctly identify survivors (True Negative Rate).
- Positive Predictive Value: 81.95%. If model predicts survival, 81.95% chance it's correct.
- Negative Predictive Value: 65.06%. If model predicts non-survival, 65.06% chance it's correct.

Statistical Tests:
- P-Value [Acc > NIR]: < 2.2e-16. Model significantly better than No Information Rate.
- Mcnemar's Test: < 2.2e-16. Significant difference between type I and type II errors.

Overall Inferences:
- Model is more effective at identifying survivors than non-survivors.
- Imbalance in sensitivity and specificity suggests potential bias towards predicting survival.
- High prevalence (74.29%) might influence the predictive values.
- Balanced Accuracy (66.78%) indicates room for improvement in managing class imbalance.

Conclusion:
- While the model is quite accurate, there's a need for improved balance in sensitivity and specificity.
- Consideration for class imbalance and further feature refinement could enhance model performance.

#### ROC Curve and AUC Analysis

<details>
<summary>Click for Code and Outputs </summary>
```{r }
# Load the pROC library
library(pROC)

# Calculate the ROC curve
roc_response <- roc(response = final_df$hospdead, predictor = as.numeric(predictions), percent = TRUE)

# Specify the thresholds you want to display on the plot
thresholds_to_print <- c(0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9)

# Plotting the ROC curve with thresholds
plot(roc_response, 
     main = "ROC Curve Analysis", 
     xlab = "False Positive Rate (1 - Specificity)", 
     ylab = "True Positive Rate (Sensitivity)", 
     col = "#1c61b6", 
     lwd = 2,
     print.thres = thresholds_to_print, 
     print.thres.pattern = "%.2f", # Adding percent sign to thresholds
     print.thres.cex = 0.6,
     print.auc = TRUE,
     print.auc.pattern = "AUC: %.3f", # Customize AUC print format
     print.auc.col = "blue",
     print.auc.cex = 1.2,
     grid = TRUE,
     identity = TRUE, # Turn off the identity line
     max.auc.polygon = TRUE, # Do not fill the area up to the maximum AUC possible
     auc.polygon = TRUE,
     auc.polygon.col = "lightblue"
)
```
<details>

#### ROC Curve and AUC Interpretation for Hospdead Variable:

- The AUC of 82.075% suggests the model is effective in distinguishing between patients who died in the hospital (hospdead = 1) and those who did not (hospdead = 0).
- The True Positive Rate (TPR) or sensitivity indicates the model's accuracy in identifying actual in-hospital deaths.
- The False Positive Rate (FPR) or 1 - specificity represents how often the model incorrectly predicts death among those who survived.

Impact of Class Distribution:
- The higher number of survivors (hospdead = 0) may influence the model's predictive performance, particularly its specificity.
- Careful threshold selection is crucial to balance TPR and FPR given the class imbalance.

Threshold Strategy for Clinical Utility:
- In clinical settings where the cost of missing an actual death (False Negative) is high, a lower threshold may be warranted to ensure high sensitivity.
- Conversely, to avoid overburdening the healthcare system with false alarms, a threshold that ensures an acceptable level of specificity without sacrificing too much sensitivity might be preferred.

Conclusion:
- The model's strong AUC performance, considering the distribution of the `hospdead` variable, indicates its potential as a supportive tool for prognostic assessment in a hospital setting.
- The plotted thresholds on the ROC curve provide insights into how different cut-off points will affect the model's prediction of in-hospital mortality.

### Final Analysis and Interpretation of Model's Performance at Threshold = 0.23

Model's Overall Accuracy:
- At 79.2%, the model performs well, with a moderate Kappa (0.3813) suggesting useful predictive power.

True/False Positive/Negative Rates:
- High Sensitivity (92.34%): Model effectively identifies true non-deaths.
- Low Specificity (41.22%): Model less effective at identifying true deaths.
- PPV (81.95%): High likelihood that a predicted non-death is correct.
- NPV (65.06%): Moderate likelihood that a predicted death is correct.

Statistical Significance:
- The model is significantly better than guessing, supported by P-Value [Acc > NIR].

Model Bias and Class Imbalance:
- The model shows a potential bias towards predicting non-deaths.
- The prevalence of non-deaths may be skewing predictive values.

Conclusions and Takeaways:
- The model's high sensitivity is beneficial for scenarios where missing a non-death is critical.
- To improve specificity, consider re-balancing the dataset or revising the feature selection.
- Given the moderate Kappa and the significant Mcnemar's Test, further model tuning is advised to better balance sensitivity and specificity.

### GLM: Poisson Regression Analysis

Research Question:
"What are the impacts of various patient characteristics and hospital variables 
on the number of comorbidities (num.co) a patient has?"

#### Poisson Distribution Visualization for num.co

<details>
<summary>Click for Code and Outputs </summary>
```{r}

# Load necessary library
library(ggplot2)

# Histogram to visualize the distribution of num.co with ticks incremented by 1
ggplot(final_df, aes(x = num.co)) +
  geom_histogram(bins = 30, fill = "skyblue", color = "black") +
  scale_x_continuous(breaks = seq(min(final_df$num.co), max(final_df$num.co), by = 1)) +
  labs(title = "Histogram of Number of Comorbidities (num.co)",
       x = "Number of Comorbidities",
       y = "Frequency") +
  theme_minimal()

```
<details>


The histogram shows that the distribution of num.co is right-skewed poisson distribution, with a long tail to the right.

### Poisson Regression Model Building and Selection

We will build our Poisson regression model starting with 'adlsc' due to its high statistical significance, and then incrementally add other variables based on their significance and impact on model fit, carefully evaluating each addition for its contribution and potential interactions.



<details>
<summary>Click for Code and Outputs </summary>
```{r}
# Building an initial Poisson regression model using all variables
# to evaluate their initial significance.
poisson_model <- glm(num.co ~ ., family = "poisson", data = final_df)

# Summary of the model
summary(poisson_model)

# Model 1: Initial Model

step1_poisson <- glm(num.co ~ adlsc, family = "poisson", data = final_df)
summary(step1_poisson)

# Model 2: Adding 'age'

step2_poisson <- update(step1_poisson, . ~ . + age)
summary(step2_poisson)

# Model 3: Adding 'bun' and 'crea'

step3_poisson <- update(step2_poisson, . ~ . + bun + crea)
summary(step3_poisson)
# crea is insignificant, so remove it from the model
step3_poisson <- update(step3_poisson, . ~ . - crea)

# Model 4: Adding 'charges'

step4_poisson <- update(step3_poisson, . ~ . + charges)
summary(step4_poisson)

# Model 5: Adding 'temp', 'bili' and 'alb'

step5_poisson <- update(step4_poisson, . ~ . + temp + bili + alb)
summary(step5_poisson)

# Model 6: Adding 'slos'

step6_poisson <- update(step5_poisson, . ~ . + slos)
summary(step6_poisson)

# After adding 'slos', 'alb' becomes insignificant, so remove it from the model 
step6_poisson <- update(step6_poisson, . ~ . - alb)
summary(step6_poisson)

# We remove 'bili' consequent to the removal of 'alb' as it is marginally insignificant now, but add 'pafi' as it is significant

step6_poisson <- update(step6_poisson, . ~ . - bili + pafi)
summary(step6_poisson)

# Start adding categorical variables starting with 'dzgroup' and 'dementia'

step7_poisson <- update(step6_poisson, . ~ . + dzgroup + dementia)
summary(step7_poisson)

# 'pafi' and 'income' are insignificant now, so remove them from the model

step7_poisson <- update(step7_poisson, . ~ . - pafi - charges)
summary(step7_poisson)

# Check 'edu' and 'scoma' for significance

step8_poisson <- update(step7_poisson, . ~ . + edu + scoma)
summary(step8_poisson)

# 'scoma' is insignificant, so remove it from the model

step8_poisson <- update(step8_poisson, . ~ . - scoma)
summary(step8_poisson)

# Add 'hospdead' to the model

step9_poisson <- update(step8_poisson, . ~ . + hospdead)
summary(step9_poisson)

# 'hospdead' is insignificant, so remove it from the model

step9_poisson <- update(step9_poisson, . ~ . - hospdead)
summary(step9_poisson)

```
<details>

### CROSS VALIDATE AND COMPARE RESULTS OF THE POISSON MODELS

Perform cross-validation for each of the Poisson regression models

<details>
<summary>Click for Code and Outputs </summary>
```{r}

# Create a function for cross-validation
library(caret)

cross_validate_poisson <- function(model_formula, full_df, number_folds = 10) {
  # Set up cross-validation control
  train_control <- trainControl(method = "cv", number = number_folds, classProbs = FALSE, summaryFunction = defaultSummary)

  # Train the model using cross-validation
  cv_model <- train(model_formula, data = full_df, method = "glm", family = "poisson", trControl = train_control)
  
  # Return the results
  return(cv_model$results)
}

# Perform cross-validation for each of the remaining models
# Set a seed for reproducibility
set.seed(123)

# Perform cross-validation for each model step
results_step1 <- cross_validate_model(step1_poisson, final_df)
results_step2 <- cross_validate_model(step2_poisson, final_df)
results_step3 <- cross_validate_model(step3_poisson, final_df)
results_step4 <- cross_validate_model(step4_poisson, final_df)
results_step5 <- cross_validate_model(step5_poisson, final_df)
results_step6 <- cross_validate_model(step6_poisson, final_df)
results_step7 <- cross_validate_model(step7_poisson, final_df)
results_step8 <- cross_validate_model(step8_poisson, final_df)
results_step9 <- cross_validate_model(step9_poisson, final_df)

# Rounding and creating a data frame for the results
results_df <- data.frame(
  Model = c("step1_poisson", "step2_poisson", "step3_poisson", 
            "step4_poisson", "step5_poisson", "step6_poisson", 
            "step7_poisson", "step8_poisson", "step9_poisson"),
  RMSE = round(c(results_step1$RMSE, results_step2$RMSE, results_step3$RMSE, results_step4$RMSE, 
                results_step5$RMSE, results_step6$RMSE, results_step7$RMSE, results_step8$RMSE, results_step9$RMSE), 3),
  Rsquared = round(c(results_step1$Rsquared, results_step2$Rsquared, results_step3$Rsquared, results_step4$Rsquared, 
                    results_step5$Rsquared, results_step6$Rsquared, results_step7$Rsquared, results_step8$Rsquared, results_step9$Rsquared), 3),
  MAE = round(c(results_step1$MAE, results_step2$MAE, results_step3$MAE, results_step4$MAE, 
               results_step5$MAE, results_step6$MAE, results_step7$MAE, results_step8$MAE, results_step9$MAE), 3)
)

# Displaying the results in a datatable
datatable(results_df, caption = 'Poisson Model Cross-validation Results', options = list(pageLength = 10))

```
<details>

#### Poisson Regression Model Development Summary:
Throughout our model development process, we focused on refining a Poisson regression model to predict the number of comorbidities:

Early Stages (Steps 1-6): We began with a simple model and incrementally added variables like 'age', 'bun', and 'charges'. This phase yielded modest improvements in RMSE and R-squared.

Significant Improvements (Steps 7-9): Introduction of categorical variables ('dzgroup', 'dementia') markedly increased R-squared, indicating their strong predictive power. Some variables became insignificant and were removed.

Optimal Model Identification: The step7_poisson model emerged as optimal, balancing complexity and accuracy with notable R-squared improvement and low RMSE/MAE. It effectively predicts the number of comorbidities while avoiding overfitting.



















